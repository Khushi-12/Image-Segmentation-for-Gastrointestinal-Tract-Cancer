{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-02-16T21:09:05.333606Z","iopub.status.busy":"2024-02-16T21:09:05.333225Z","iopub.status.idle":"2024-02-16T21:09:05.338442Z","shell.execute_reply":"2024-02-16T21:09:05.337501Z","shell.execute_reply.started":"2024-02-16T21:09:05.333567Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: albumentations in ./.venv/lib/python3.9/site-packages (1.4.2)\n","Requirement already satisfied: numpy>=1.24.4 in ./.venv/lib/python3.9/site-packages (from albumentations) (1.26.4)\n","Requirement already satisfied: scipy>=1.10.0 in ./.venv/lib/python3.9/site-packages (from albumentations) (1.10.0)\n","Requirement already satisfied: scikit-image>=0.21.0 in ./.venv/lib/python3.9/site-packages (from albumentations) (0.22.0)\n","Requirement already satisfied: PyYAML in ./.venv/lib/python3.9/site-packages (from albumentations) (6.0.1)\n","Requirement already satisfied: typing-extensions>=4.9.0 in ./.venv/lib/python3.9/site-packages (from albumentations) (4.10.0)\n","Requirement already satisfied: scikit-learn>=1.3.2 in ./.venv/lib/python3.9/site-packages (from albumentations) (1.4.1.post1)\n","Requirement already satisfied: opencv-python-headless>=4.9.0 in ./.venv/lib/python3.9/site-packages (from albumentations) (4.9.0.80)\n","Requirement already satisfied: networkx>=2.8 in ./.venv/lib/python3.9/site-packages (from scikit-image>=0.21.0->albumentations) (3.2.1)\n","Requirement already satisfied: pillow>=9.0.1 in ./.venv/lib/python3.9/site-packages (from scikit-image>=0.21.0->albumentations) (9.4.0)\n","Requirement already satisfied: imageio>=2.27 in ./.venv/lib/python3.9/site-packages (from scikit-image>=0.21.0->albumentations) (2.34.0)\n","Requirement already satisfied: tifffile>=2022.8.12 in ./.venv/lib/python3.9/site-packages (from scikit-image>=0.21.0->albumentations) (2024.2.12)\n","Requirement already satisfied: packaging>=21 in ./.venv/lib/python3.9/site-packages (from scikit-image>=0.21.0->albumentations) (24.0)\n","Requirement already satisfied: lazy_loader>=0.3 in ./.venv/lib/python3.9/site-packages (from scikit-image>=0.21.0->albumentations) (0.3)\n","Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.9/site-packages (from scikit-learn>=1.3.2->albumentations) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in ./.venv/lib/python3.9/site-packages (from scikit-learn>=1.3.2->albumentations) (3.4.0)\n"]}],"source":["# Imports\n","!pip install albumentations"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting torchsummary\n","  Downloading torchsummary-1.5.1-py3-none-any.whl.metadata (296 bytes)\n","Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\n","Installing collected packages: torchsummary\n","Successfully installed torchsummary-1.5.1\n"]}],"source":["!pip install torchsummary"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: rasterio in ./.venv/lib/python3.9/site-packages (1.3.9)\n","Requirement already satisfied: affine in ./.venv/lib/python3.9/site-packages (from rasterio) (2.4.0)\n","Requirement already satisfied: attrs in ./.venv/lib/python3.9/site-packages (from rasterio) (23.2.0)\n","Requirement already satisfied: certifi in ./.venv/lib/python3.9/site-packages (from rasterio) (2024.2.2)\n","Requirement already satisfied: click>=4.0 in ./.venv/lib/python3.9/site-packages (from rasterio) (8.1.7)\n","Requirement already satisfied: cligj>=0.5 in ./.venv/lib/python3.9/site-packages (from rasterio) (0.7.2)\n","Requirement already satisfied: numpy in ./.venv/lib/python3.9/site-packages (from rasterio) (1.26.4)\n","Requirement already satisfied: snuggs>=1.4.1 in ./.venv/lib/python3.9/site-packages (from rasterio) (1.4.7)\n","Requirement already satisfied: click-plugins in ./.venv/lib/python3.9/site-packages (from rasterio) (1.1.1)\n","Requirement already satisfied: setuptools in ./.venv/lib/python3.9/site-packages (from rasterio) (58.0.4)\n","Requirement already satisfied: pyparsing>=2.1.6 in ./.venv/lib/python3.9/site-packages (from snuggs>=1.4.1->rasterio) (3.1.2)\n"]}],"source":["!pip install rasterio"]},{"cell_type":"code","execution_count":24,"metadata":{"_kg_hide-input":false,"execution":{"iopub.execute_input":"2024-02-16T21:49:20.707634Z","iopub.status.busy":"2024-02-16T21:49:20.706685Z","iopub.status.idle":"2024-02-16T21:49:20.722564Z","shell.execute_reply":"2024-02-16T21:49:20.721575Z","shell.execute_reply.started":"2024-02-16T21:49:20.707594Z"},"trusted":true},"outputs":[],"source":["# Imports\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","tqdm.pandas()\n","import multiprocessing\n","import glob\n","import re\n","import plotly.express as pxy\n","from matplotlib.patches import Rectangle\n","from matplotlib.colors import LinearSegmentedColormap\n","import torch\n","\n","pd.options.plotting.backend = \"plotly\"\n","import random\n","from glob import glob\n","import os, shutil\n","import time\n","import copy\n","import joblib\n","from collections import defaultdict\n","import gc\n","from IPython import display as ipd\n","from torch.optim import lr_scheduler\n","\n","# visualization\n","import cv2\n","import matplotlib.pyplot as plt\n","from matplotlib.patches import Rectangle\n","\n","# Sklearn\n","from sklearn.model_selection import StratifiedKFold, KFold, StratifiedGroupKFold\n","\n","# PyTorch \n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","from torch.utils.data import Dataset, DataLoader\n","from torch.cuda import amp\n","import segmentation_models_pytorch as smp\n","import timm\n","\n","# Albumentations for augmentations\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","\n","import rasterio\n","from joblib import Parallel, delayed\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","# For descriptive error messages\n","os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n","\n","\n","#Importing Custom u-net model\n","import u_net\n","\n","\n","seed          = 42\n","model_name    = 'Unet'\n","train_bs      = 32\n","valid_bs      = train_bs*2\n","img_size      = (224, 224)\n","epochs        = 5\n","lr            = 2e-3\n","scheduler     = 'CosineAnnealingLR'\n","min_lr        = 1e-6\n","T_max         = int(30000/train_bs*epochs)+50\n","T_0           = 25\n","warmup_epochs = 0\n","wd            = 1e-6\n","n_accumulate  = max(1, 32//train_bs)\n","n_fold        = 5\n","fold_selected = 1\n","num_classes   = 3\n","device        = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["multiprocessing.set_start_method(\"fork\")"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-02-16T21:49:21.214051Z","iopub.status.busy":"2024-02-16T21:49:21.213667Z","iopub.status.idle":"2024-02-16T21:49:21.530473Z","shell.execute_reply":"2024-02-16T21:49:21.529450Z","shell.execute_reply.started":"2024-02-16T21:49:21.214019Z"},"trusted":true},"outputs":[],"source":["train_df = pd.read_csv(\"/Users/khushi/Downloads/uw-madison-gi-tract-image-segmentation/train.csv\")"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-02-16T21:49:22.086127Z","iopub.status.busy":"2024-02-16T21:49:22.085726Z","iopub.status.idle":"2024-02-16T21:49:22.126308Z","shell.execute_reply":"2024-02-16T21:49:22.125396Z","shell.execute_reply.started":"2024-02-16T21:49:22.086096Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 115488 entries, 0 to 115487\n","Data columns (total 3 columns):\n"," #   Column        Non-Null Count   Dtype \n","---  ------        --------------   ----- \n"," 0   id            115488 non-null  object\n"," 1   class         115488 non-null  object\n"," 2   segmentation  33913 non-null   object\n","dtypes: object(3)\n","memory usage: 2.6+ MB\n"]}],"source":["train_df.info()"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-02-16T21:49:22.734059Z","iopub.status.busy":"2024-02-16T21:49:22.733661Z","iopub.status.idle":"2024-02-16T21:49:22.860829Z","shell.execute_reply":"2024-02-16T21:49:22.859923Z","shell.execute_reply.started":"2024-02-16T21:49:22.734025Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>class</th>\n","      <th>segmentation</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>115488</td>\n","      <td>115488</td>\n","      <td>33913</td>\n","    </tr>\n","    <tr>\n","      <th>unique</th>\n","      <td>38496</td>\n","      <td>3</td>\n","      <td>33899</td>\n","    </tr>\n","    <tr>\n","      <th>top</th>\n","      <td>case123_day20_slice_0001</td>\n","      <td>large_bowel</td>\n","      <td>12629 10 12894 12 13158 15 13423 17 13688 19 1...</td>\n","    </tr>\n","    <tr>\n","      <th>freq</th>\n","      <td>3</td>\n","      <td>38496</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                              id        class  \\\n","count                     115488       115488   \n","unique                     38496            3   \n","top     case123_day20_slice_0001  large_bowel   \n","freq                           3        38496   \n","\n","                                             segmentation  \n","count                                               33913  \n","unique                                              33899  \n","top     12629 10 12894 12 13158 15 13423 17 13688 19 1...  \n","freq                                                    2  "]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["train_df.describe()"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-02-16T21:49:23.546814Z","iopub.status.busy":"2024-02-16T21:49:23.546476Z","iopub.status.idle":"2024-02-16T21:49:23.573505Z","shell.execute_reply":"2024-02-16T21:49:23.572461Z","shell.execute_reply.started":"2024-02-16T21:49:23.546786Z"},"trusted":true},"outputs":[],"source":["# Utility Functions\n","def get_image_path(base_path, df):\n","    '''Gets the case, day, slice_no and path of the dataset (either train or test).\n","    base_path: path to train image folder\n","    return :: modified dataframe'''\n","    \n","    digit_pat = r'[0-9]+'\n","    # Create case, day and slice columns\n","    df[\"case\"] = df['id'].apply(lambda x: re.findall(digit_pat, x.split('_')[0])[0]) # df[\"id\"].apply(lambda x: x.split(\"_\")[0])\n","    df[\"day\"] = df['id'].apply(lambda x: re.findall(digit_pat, x.split('_')[1])[0])  # df[\"id\"].apply(lambda x: x.split(\"_\")[1])\n","    df[\"slice_no\"] = df[\"id\"].apply(lambda x: x.split(\"_\")[-1])\n","\n","    df[\"path\"] = 0\n","    \n","    n = len(df)\n","\n","    # Loop through entire dataset\n","    for k in tqdm(range(n)):\n","        data = df.iloc[k, :]\n","        segmentation = data.segmentation\n","\n","        # In case coordinates for healthy tissue are present\n","        case = \"case\"+data.case\n","        day = 'day'+data.day\n","        slice_no = data.slice_no\n","        # Change value to the correct one\n","        df.loc[k, \"path\"] = glob(f\"{base_path}/{case}/{case}_{day}/scans/slice_{slice_no}*\")[0]\n","    return df\n","\n","\n","def get_img_size(x, flag):\n","    \n","    if x != 0:\n","        split = x.split(\"_\")\n","        width = split[3]\n","        height = split[4]\n","    \n","        if flag == \"width\":\n","            return int(width)\n","        elif flag == \"height\":\n","            return int(height)\n","    \n","    return 0\n","\n","def get_pixel_size(x, flag):\n","    \n","    if x != 0:\n","        split = x.split(\"_\")\n","        width = split[-2]\n","        height = \".\".join(split[-1].split(\".\")[:-1])\n","    \n","        if flag == \"width\":\n","            return float(width)\n","        elif flag == \"height\":\n","            return float(height)\n","    \n","    return 0\n","\n","def CustomCmap(rgb_color):\n","\n","    r1,g1,b1 = rgb_color\n","\n","    cdict = {'red': ((0, r1, r1),\n","                   (1, r1, r1)),\n","           'green': ((0, g1, g1),\n","                    (1, g1, g1)),\n","           'blue': ((0, b1, b1),\n","                   (1, b1, b1))}\n","\n","    cmap = LinearSegmentedColormap('custom_cmap', cdict)\n","    return cmap\n","\n","\n","def show_sample_images(sample_paths):\n","    '''Displays simple images (without mask).'''\n","\n","    # Get additional info from the path\n","    case_name = [info.split(\"_\")[0][-7:] for info in sample_paths]\n","    day_name = [info.split(\"_\")[1].split(\"/\")[0] for info in sample_paths]\n","    slice_name = [info.split(\"_\")[2] for info in sample_paths]\n","\n","\n","    # Plot\n","    fig, axs = plt.subplots(2, 5, figsize=(23, 8))\n","    axs = axs.flatten()\n","\n","    for k, path in enumerate(sample_paths):\n","        \n","        title = f\"{k+1}. {case_name[k]} - {day_name[k]} - {slice_name[k]}\"\n","        axs[k].set_title(title, fontsize = 14, \n","                         color = my_colors[-1], weight='bold')\n","        axs[k].imshow(img)\n","        axs[k].axis(\"off\")\n","\n","    plt.tight_layout()\n","    plt.show()\n","    \n","def mask_from_segmentation(segment, shape):\n","    segm = np.asarray(segment.split(), dtype=int)\n","\n","   \n","    # Get start point and length between points\n","    start_point = segm[0::2] - 1\n","    length_point = segm[1::2]\n","\n","    # Compute the location of each endpoint\n","    end_point = start_point + length_point\n","\n","    # Create an empty list mask the size of the original image\n","    # take onl\n","    case_mask = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n","\n","    # Change pixels from 0 to 1 that are within the segmentation\n","    for start, end in zip(start_point, end_point):\n","        case_mask[start:end] = 1\n","\n","    case_mask = case_mask.reshape((shape[0], shape[1]))    \n","    return case_mask\n","\n","def plot_original_mask(img, mask, alpha=1):\n","\n","    # Change pixels - when 1 make True, when 0 make NA\n","    mask = np.ma.masked_where(mask == 0, mask)\n","\n","    # Split the channels\n","    mask_largeB = mask[:, :, 0]\n","    mask_smallB = mask[:, :, 1]\n","    mask_stomach = mask[:, :, 2]\n","\n","    # Plot the 2 images (Original and with Mask)\n","    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 10))\n","\n","    # Original\n","    ax1.set_title(\"Original Image\")\n","    ax1.imshow(img)\n","    ax1.axis(\"off\")\n","\n","    # With Mask\n","    ax2.set_title(\"Image with Mask\")\n","    ax2.imshow(img)\n","    ax2.imshow(mask_largeB, interpolation='none', cmap=CMAP1, alpha=alpha)\n","    ax2.imshow(mask_smallB, interpolation='none', cmap=CMAP2, alpha=alpha)\n","    ax2.imshow(mask_stomach, interpolation='none',cmap=CMAP3, alpha=alpha)\n","    ax2.legend(legend_colors, ['large_bowel', 'small_bowel', 'stomach'])\n","    ax2.axis(\"off\")\n","    \n","#     fig.savefig('foo.png', dpi=500)\n","    plt.show()"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-02-16T21:49:24.422078Z","iopub.status.busy":"2024-02-16T21:49:24.421614Z","iopub.status.idle":"2024-02-16T21:49:24.430341Z","shell.execute_reply":"2024-02-16T21:49:24.429229Z","shell.execute_reply.started":"2024-02-16T21:49:24.422040Z"},"trusted":true},"outputs":[],"source":["mask_colors = [(1.0, 0.7, 0.1), (1.0, 0.5, 1.0), (1.0, 0.22, 0.099)]\n","legend_colors = [Rectangle((0,0),1,1, color=color) for color in mask_colors]\n","labels = [\"Large Bowel\", \"Small Bowel\", \"Stomach\"]\n","\n","CMAP1 = CustomCmap(mask_colors[0])\n","CMAP2 = CustomCmap(mask_colors[1])\n","CMAP3 = CustomCmap(mask_colors[2])"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-02-16T21:49:25.197881Z","iopub.status.busy":"2024-02-16T21:49:25.197506Z","iopub.status.idle":"2024-02-16T21:51:09.196228Z","shell.execute_reply":"2024-02-16T21:51:09.194797Z","shell.execute_reply.started":"2024-02-16T21:49:25.197829Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 115488/115488 [00:35<00:00, 3256.50it/s]\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>large_bowel</th>\n","      <th>small_bowel</th>\n","      <th>stomach</th>\n","      <th>path</th>\n","      <th>case</th>\n","      <th>day</th>\n","      <th>slice</th>\n","      <th>width</th>\n","      <th>height</th>\n","      <th>count</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>4228</th>\n","      <td>case145_day19_slice_0053</td>\n","      <td></td>\n","      <td></td>\n","      <td>44155 1 44513 6 44871 9 45230 11 45589 13 4594...</td>\n","      <td>/Users/khushi/Downloads/uw-madison-gi-tract-im...</td>\n","      <td>145</td>\n","      <td>19</td>\n","      <td>0</td>\n","      <td>360</td>\n","      <td>310</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>17556</th>\n","      <td>case7_day0_slice_0021</td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td>/Users/khushi/Downloads/uw-madison-gi-tract-im...</td>\n","      <td>7</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>266</td>\n","      <td>266</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3426</th>\n","      <td>case88_day0_slice_0115</td>\n","      <td>20675 2 21033 7 21389 12 21505 5 21742 19 2186...</td>\n","      <td>17117 6 17468 16 17826 19 18184 23 18543 27 18...</td>\n","      <td></td>\n","      <td>/Users/khushi/Downloads/uw-madison-gi-tract-im...</td>\n","      <td>88</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>360</td>\n","      <td>310</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>25776</th>\n","      <td>case55_day20_slice_0065</td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td>/Users/khushi/Downloads/uw-madison-gi-tract-im...</td>\n","      <td>55</td>\n","      <td>20</td>\n","      <td>0</td>\n","      <td>266</td>\n","      <td>266</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>6152</th>\n","      <td>case91_day0_slice_0025</td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td>/Users/khushi/Downloads/uw-madison-gi-tract-im...</td>\n","      <td>91</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>266</td>\n","      <td>266</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                             id  \\\n","4228   case145_day19_slice_0053   \n","17556     case7_day0_slice_0021   \n","3426     case88_day0_slice_0115   \n","25776   case55_day20_slice_0065   \n","6152     case91_day0_slice_0025   \n","\n","                                             large_bowel  \\\n","4228                                                       \n","17556                                                      \n","3426   20675 2 21033 7 21389 12 21505 5 21742 19 2186...   \n","25776                                                      \n","6152                                                       \n","\n","                                             small_bowel  \\\n","4228                                                       \n","17556                                                      \n","3426   17117 6 17468 16 17826 19 18184 23 18543 27 18...   \n","25776                                                      \n","6152                                                       \n","\n","                                                 stomach  \\\n","4228   44155 1 44513 6 44871 9 45230 11 45589 13 4594...   \n","17556                                                      \n","3426                                                       \n","25776                                                      \n","6152                                                       \n","\n","                                                    path  case day slice  \\\n","4228   /Users/khushi/Downloads/uw-madison-gi-tract-im...   145  19     0   \n","17556  /Users/khushi/Downloads/uw-madison-gi-tract-im...     7   0     0   \n","3426   /Users/khushi/Downloads/uw-madison-gi-tract-im...    88   0     0   \n","25776  /Users/khushi/Downloads/uw-madison-gi-tract-im...    55  20     0   \n","6152   /Users/khushi/Downloads/uw-madison-gi-tract-im...    91   0     0   \n","\n","       width  height  count  \n","4228     360     310      1  \n","17556    266     266      0  \n","3426     360     310      2  \n","25776    266     266      0  \n","6152     266     266      0  "]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["base_path = \"/Users/khushi/Downloads/uw-madison-gi-tract-image-segmentation/train\"\n","\n","# # Prep and save file\n","train_data = get_image_path(base_path, df=train_df)\n","\n","train_data[\"image_width\"] = train_data[\"path\"].apply(lambda x: int(x[:-4].rsplit(\"_\",4)[1]))\n","train_data[\"image_height\"] = train_data[\"path\"].apply(lambda x: int(x[:-4].rsplit(\"_\",4)[2]))\n","\n","train_data[\"pixel_width\"] = train_data[\"path\"].apply(lambda x: int(x[:-4].rsplit(\"_\",4)[1]))\n","train_data[\"pixel_height\"] = train_data[\"path\"].apply(lambda x: get_pixel_size(x, \"height\"))\n","\n","train_data['slice_no'] = train_data['slice_no'].apply(lambda x: x[0])\n","\n","\n","train_data['case'] = train_data['case'].astype(int)\n","\n","df_train = pd.DataFrame({'id':train_data['id'][::3]})\n","\n","df_train['large_bowel'] = train_data['segmentation'][::3].values\n","df_train['small_bowel'] = train_data['segmentation'][1::3].values\n","df_train['stomach'] = train_data['segmentation'][2::3].values\n","\n","df_train['path'] = train_data['path'][::3].values\n","df_train['case'] = train_data['case'][::3].values\n","df_train['day'] = train_data['day'][::3].values\n","df_train['slice'] = train_data['slice_no'][::3].values\n","df_train['width'] = train_data['image_width'][::3].values\n","df_train['height'] = train_data['image_height'][::3].values\n","\n","\n","df_train.reset_index(inplace=True,drop=True)\n","df_train.fillna('',inplace=True); \n","df_train['count'] = np.sum(df_train.iloc[:,1:4]!='',axis=1).values\n","df_train.sample(5)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-02-16T21:51:09.198215Z","iopub.status.busy":"2024-02-16T21:51:09.197923Z","iopub.status.idle":"2024-02-16T21:51:09.207131Z","shell.execute_reply":"2024-02-16T21:51:09.206081Z","shell.execute_reply.started":"2024-02-16T21:51:09.198190Z"},"trusted":true},"outputs":[],"source":["def rle_decode(mask_rle, shape):\n","    '''\n","    mask_rle: run-length as string formated (start length)\n","    shape: (height,width) of array to return\n","    Returns numpy array, 1 - mask, 0 - background\n","    '''\n","    s = mask_rle.split()\n","    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n","    starts -= 1\n","    ends = starts + lengths\n","    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n","    for lo, hi in zip(starts, ends):\n","        img[lo:hi] = 1\n","    return img.reshape(shape)  # Needed to align to RLE direction\n","\n","\n","# ref.: https://www.kaggle.com/stainsby/fast-tested-rle\n","def rle_encode(img):\n","    '''\n","    img: numpy array, 1 - mask, 0 - background\n","    Returns run length as string formated\n","    '''\n","    pixels = img.flatten()\n","    pixels = np.concatenate([[0], pixels, [0]])\n","    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n","    runs[1::2] -= runs[::2]\n","    return ' '.join(str(x) for x in runs)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-02-16T21:51:09.208875Z","iopub.status.busy":"2024-02-16T21:51:09.208559Z","iopub.status.idle":"2024-02-16T21:51:09.226362Z","shell.execute_reply":"2024-02-16T21:51:09.225358Z","shell.execute_reply.started":"2024-02-16T21:51:09.208833Z"},"trusted":true},"outputs":[],"source":["class BuildDataset(torch.utils.data.Dataset):\n","    def __init__(self, df, subset='train', transforms=None):\n","        self.df = df\n","        self.subset = subset\n","        self.transforms = transforms\n","    \n","    def __len__(self):\n","        return len(self.df)\n","    \n","    def __getitem__(self, index):\n","        masks = np.zeros((224,224,3), dtype=np.float32)\n","        img_path = self.df['path'].iloc[index]\n","        w = self.df['width'].iloc[index]\n","        h = self.df['height'].iloc[index]\n","        img = self.__load_img(img_path)\n","        if self.subset=='train':\n","            for k,j in zip([0,1,2],[\"large_bowel\",\"small_bowel\",\"stomach\"]):\n","                rles=self.df[j].iloc[index]\n","                mask = rle_decode(rles, shape=(h, w, 1))\n","                mask = cv2.resize(mask, (224,224))\n","                masks[:,:,k] = mask\n","        \n","        masks = masks.transpose(2, 0, 1)\n","        img = img.transpose(2, 0, 1)\n","        \n","        if self.subset=='train': return torch.tensor(img), torch.tensor(masks)\n","        else: return torch.tensor(img)\n","\n","    def __load_img(self, img_path):\n","        img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n","        img = (img - img.min())/(img.max() - img.min())*255.0 \n","        img = cv2.resize(img, (224,224))\n","        img = np.tile(img[...,None], [1, 1, 3]) # gray to rgb\n","        img = img.astype(np.float32) /255.\n","        return img\n","    \n","    def get_id_mask(self, idx, verbose=False):\n","        '''Returns a mask for each case ID. If no segmentation was found, the mask will be empty\n","        - meaning formed by only 0\n","        ID: the case ID from the train.csv file\n","        verbose: True if we want any prints\n","        return: segmentation mask'''\n","\n","        train = self.df\n","        \n","        # ~~~ Get the data ~~~\n","        # Get the portion of dataframe where we have ONLY the speciffied ID\n","        #index_data = train[train['id']==id].reset_index(drop=True)\n","        for row in train[train.index==idx].iterrows():\n","            row = row[1]\n","\n","            # Split the dataframe into 3 series of observations\n","            # each for one speciffic class - \"large_bowel\", \"small_bowel\", \"stomach\"\n","            observations = [index_data.loc[k, :] for k in range(3)]\n","        \n","        # ~~~ Create the mask ~~~\n","        # Get the maximum height out of all observations\n","        # if max == 0 then no class has a segmentation\n","        # otherwise we keep the length of the mask\n","        max_height = np.max([obs.image_height for obs in observations])\n","        max_width = np.max([obs.image_width for obs in observations])\n","\n","        # Get shape of the image\n","        # 3 channels of color/classes\n","        shape = (max_height, max_width, 3)\n","\n","        # Create an empty mask with the shape of the image\n","        mask = np.zeros(shape, dtype=np.uint8)\n","\n","        # If there is at least 1 segmentation found in the group of 3 classes\n","        if max_height != 0:\n","            for k, location in enumerate([\"large_bowel\", \"small_bowel\", \"stomach\"]):\n","                observation = observations[k]\n","                segmentation = observation.segmentation\n","\n","                # If a segmentation is found\n","                # Append a new channel to the mask\n","                if pd.isnull(segmentation) == False:\n","                    mask[..., k] = mask_from_segmentation(segmentation, shape)\n","\n","        return mask"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-02-16T21:53:54.032927Z","iopub.status.busy":"2024-02-16T21:53:54.032007Z","iopub.status.idle":"2024-02-16T21:53:54.038804Z","shell.execute_reply":"2024-02-16T21:53:54.037717Z","shell.execute_reply.started":"2024-02-16T21:53:54.032891Z"},"trusted":true},"outputs":[],"source":["data_transforms = {\n","    \"train\": A.Compose([\n","        A.Resize(224,224, interpolation=cv2.INTER_NEAREST),\n","        A.Blur(blur_limit=(5, 5), p=1.0),\n","        A.Normalize(mean=0.5, std=2, max_pixel_value=255.0)], p=1.0),\n","    \"valid\": A.Compose([\n","        A.Resize(224,224, interpolation=cv2.INTER_NEAREST),\n","        ], p=1.0)\n","}"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-02-16T21:53:55.325505Z","iopub.status.busy":"2024-02-16T21:53:55.324865Z","iopub.status.idle":"2024-02-16T21:53:55.494186Z","shell.execute_reply":"2024-02-16T21:53:55.493287Z","shell.execute_reply.started":"2024-02-16T21:53:55.325473Z"},"trusted":true},"outputs":[],"source":["skf = StratifiedGroupKFold(n_splits=n_fold, shuffle=True, random_state=42)\n","for fold, (_, val_idx) in enumerate(skf.split(X=df_train, y=df_train['count'],groups =df_train['case']), 1):\n","    df_train.loc[val_idx, 'fold'] = fold\n","\n","df_train['fold'] = df_train['fold'].astype(np.uint8)\n","\n","train_ids = df_train[df_train[\"fold\"]!=fold_selected].index\n","valid_ids = df_train[df_train[\"fold\"]==fold_selected].index\n","\n","# train_ids = train_data.index[:30000]\n","# valid_ids = train_data.index[30000:]"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-02-16T21:59:46.206027Z","iopub.status.busy":"2024-02-16T21:59:46.205544Z","iopub.status.idle":"2024-02-16T21:59:47.120525Z","shell.execute_reply":"2024-02-16T21:59:47.119420Z","shell.execute_reply.started":"2024-02-16T21:59:46.205983Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(torch.Size([32, 3, 224, 224]), torch.Size([32, 3, 224, 224]))"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["train_dataset = BuildDataset(df_train[df_train.index.isin(train_ids)], transforms=data_transforms['train'])\n","valid_dataset = BuildDataset(df_train[df_train.index.isin(valid_ids)], transforms=data_transforms['valid'])\n","\n","train_loader = DataLoader(train_dataset,batch_size=32, num_workers=4, shuffle=True, pin_memory=True, drop_last=False)\n","\n","valid_loader = DataLoader(valid_dataset, batch_size=64,num_workers=4, shuffle=False, pin_memory=True)\n","\n","imgs, msks = next(iter(train_loader))\n","imgs.size(), msks.size()"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["pre_trained=False"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-02-16T21:59:52.232447Z","iopub.status.busy":"2024-02-16T21:59:52.232058Z","iopub.status.idle":"2024-02-16T21:59:52.624245Z","shell.execute_reply":"2024-02-16T21:59:52.623395Z","shell.execute_reply.started":"2024-02-16T21:59:52.232414Z"},"tags":["Pre-trained"],"trusted":true},"outputs":[],"source":["if pre_trained==True:\n","    ENCODER = 'efficientnet-b4'\n","    ENCODER_WEIGHTS = 'imagenet'\n","    CLASSES = ['large_bowel', 'small_bowel', 'stomach']\n","    ACTIVATION = 'softmax' # could be None for logits or 'softmax2d' for multiclass segmentation\n","\n","    # create segmentation model with pretrained encoder\n","    model = smp.Unet(\n","        encoder_name=ENCODER,\n","        encoder_weights=ENCODER_WEIGHTS,\n","        classes=len(CLASSES),\n","        activation=ACTIVATION,\n","    )\n","\n","    preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)\n","else:\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","    # model = UNet(in_channels=3, out_channels=1)\n","    model=u_net.UNet()\n","    model.to(device)"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["from torchsummary import summary"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1         [-1, 64, 224, 224]           1,792\n","       BatchNorm2d-2         [-1, 64, 224, 224]             128\n","              ReLU-3         [-1, 64, 224, 224]               0\n","            Conv2d-4         [-1, 64, 224, 224]          36,928\n","       BatchNorm2d-5         [-1, 64, 224, 224]             128\n","              ReLU-6         [-1, 64, 224, 224]               0\n","        DoubleConv-7         [-1, 64, 224, 224]               0\n","         MaxPool2d-8         [-1, 64, 112, 112]               0\n","            Conv2d-9        [-1, 128, 112, 112]          73,856\n","      BatchNorm2d-10        [-1, 128, 112, 112]             256\n","             ReLU-11        [-1, 128, 112, 112]               0\n","           Conv2d-12        [-1, 128, 112, 112]         147,584\n","      BatchNorm2d-13        [-1, 128, 112, 112]             256\n","             ReLU-14        [-1, 128, 112, 112]               0\n","       DoubleConv-15        [-1, 128, 112, 112]               0\n","        MaxPool2d-16          [-1, 128, 56, 56]               0\n","           Conv2d-17          [-1, 256, 56, 56]         295,168\n","      BatchNorm2d-18          [-1, 256, 56, 56]             512\n","             ReLU-19          [-1, 256, 56, 56]               0\n","           Conv2d-20          [-1, 256, 56, 56]         590,080\n","      BatchNorm2d-21          [-1, 256, 56, 56]             512\n","             ReLU-22          [-1, 256, 56, 56]               0\n","       DoubleConv-23          [-1, 256, 56, 56]               0\n","        MaxPool2d-24          [-1, 256, 28, 28]               0\n","           Conv2d-25          [-1, 512, 28, 28]       1,180,160\n","      BatchNorm2d-26          [-1, 512, 28, 28]           1,024\n","             ReLU-27          [-1, 512, 28, 28]               0\n","           Conv2d-28          [-1, 512, 28, 28]       2,359,808\n","      BatchNorm2d-29          [-1, 512, 28, 28]           1,024\n","             ReLU-30          [-1, 512, 28, 28]               0\n","       DoubleConv-31          [-1, 512, 28, 28]               0\n","        MaxPool2d-32          [-1, 512, 14, 14]               0\n","           Conv2d-33         [-1, 1024, 14, 14]       4,719,616\n","      BatchNorm2d-34         [-1, 1024, 14, 14]           2,048\n","             ReLU-35         [-1, 1024, 14, 14]               0\n","           Conv2d-36         [-1, 1024, 14, 14]       9,438,208\n","      BatchNorm2d-37         [-1, 1024, 14, 14]           2,048\n","             ReLU-38         [-1, 1024, 14, 14]               0\n","       DoubleConv-39         [-1, 1024, 14, 14]               0\n","  ConvTranspose2d-40          [-1, 512, 28, 28]       2,097,664\n","           Conv2d-41          [-1, 512, 28, 28]       4,719,104\n","      BatchNorm2d-42          [-1, 512, 28, 28]           1,024\n","             ReLU-43          [-1, 512, 28, 28]               0\n","           Conv2d-44          [-1, 512, 28, 28]       2,359,808\n","      BatchNorm2d-45          [-1, 512, 28, 28]           1,024\n","             ReLU-46          [-1, 512, 28, 28]               0\n","       DoubleConv-47          [-1, 512, 28, 28]               0\n","  ConvTranspose2d-48          [-1, 256, 56, 56]         524,544\n","           Conv2d-49          [-1, 256, 56, 56]       1,179,904\n","      BatchNorm2d-50          [-1, 256, 56, 56]             512\n","             ReLU-51          [-1, 256, 56, 56]               0\n","           Conv2d-52          [-1, 256, 56, 56]         590,080\n","      BatchNorm2d-53          [-1, 256, 56, 56]             512\n","             ReLU-54          [-1, 256, 56, 56]               0\n","       DoubleConv-55          [-1, 256, 56, 56]               0\n","  ConvTranspose2d-56        [-1, 128, 112, 112]         131,200\n","           Conv2d-57        [-1, 128, 112, 112]         295,040\n","      BatchNorm2d-58        [-1, 128, 112, 112]             256\n","             ReLU-59        [-1, 128, 112, 112]               0\n","           Conv2d-60        [-1, 128, 112, 112]         147,584\n","      BatchNorm2d-61        [-1, 128, 112, 112]             256\n","             ReLU-62        [-1, 128, 112, 112]               0\n","       DoubleConv-63        [-1, 128, 112, 112]               0\n","  ConvTranspose2d-64         [-1, 64, 224, 224]          32,832\n","           Conv2d-65         [-1, 64, 224, 224]          73,792\n","      BatchNorm2d-66         [-1, 64, 224, 224]             128\n","             ReLU-67         [-1, 64, 224, 224]               0\n","           Conv2d-68         [-1, 64, 224, 224]          36,928\n","      BatchNorm2d-69         [-1, 64, 224, 224]             128\n","             ReLU-70         [-1, 64, 224, 224]               0\n","       DoubleConv-71         [-1, 64, 224, 224]               0\n","           Conv2d-72          [-1, 3, 224, 224]             195\n","================================================================\n","Total params: 31,043,651\n","Trainable params: 31,043,651\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.57\n","Forward/backward pass size (MB): 712.41\n","Params size (MB): 118.42\n","Estimated Total Size (MB): 831.41\n","----------------------------------------------------------------\n"]}],"source":["summary(model,(3,224,224))"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-02-16T22:19:54.878088Z","iopub.status.busy":"2024-02-16T22:19:54.877587Z","iopub.status.idle":"2024-02-16T22:19:54.898058Z","shell.execute_reply":"2024-02-16T22:19:54.897072Z","shell.execute_reply.started":"2024-02-16T22:19:54.878054Z"},"trusted":true},"outputs":[],"source":["device = device\n","DiceLoss    = smp.losses.DiceLoss(mode='multilabel').to(device)\n","BCELoss     = smp.losses.SoftBCEWithLogitsLoss()\n","JaccardLoss = smp.losses.JaccardLoss(mode='multilabel')\n","\n","from scipy.spatial.distance import directed_hausdorff\n","\n","def dice_coef(y_true, y_pred, thr=0.5, dim=(2,3), epsilon=0.001):\n","    y_true = y_true.to(torch.float32)\n","    y_pred = (y_pred>thr).to(torch.float32)\n","    inter = (y_true*y_pred).sum(dim=dim)\n","    den = y_true.sum(dim=dim) + y_pred.sum(dim=dim)\n","    dice = ((2*inter)/(den+epsilon)).mean(dim=(1,0))\n","    return dice\n","\n","\n","\n","# The training loss goes to nan while evaluating the model through this hausdorff distance.\n","def hausdorff_distance(y_true, y_pred):\n","    difference = y_true - y_pred\n","    \n","    # Square distances using PyTorch einsum\n","    square_distances = torch.einsum(\"...i,...i->...\", difference, difference)\n","    \n","    minimum_square_distance_a_to_b = torch.min(square_distances, dim=-1)[0]\n","    \n","    # Here we are outputting the mean hausdorff distance.\n","    return torch.mean(torch.sqrt(torch.max(minimum_square_distance_a_to_b, dim=-1).values))\n","\n","def iou_coef(y_true, y_pred, thr=0.5, dim=(2,3), epsilon=0.001):\n","    y_true = y_true.to(torch.float32)\n","    y_pred = (y_pred>thr).to(torch.float32)\n","    inter = (y_true*y_pred).sum(dim=dim)\n","    union = (y_true + y_pred - y_true*y_pred).sum(dim=dim)\n","    iou = ((inter+epsilon)/(union+epsilon)).mean(dim=(1,0))\n","    return iou\n","\n","def criterion(y_pred, y_true):\n","    return 0.6*BCELoss(y_true, y_pred) + 0.4*DiceLoss(y_pred, y_true)\n","\n","def train_one_epoch(model, optimizer, scheduler, dataloader, device, epoch):\n","    model.to(device)\n","    model.train()\n","    scaler = amp.GradScaler()\n","    \n","    dataset_size = 0\n","    running_loss = 0.0\n","    \n","    pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc='Train ')\n","    for step, (images, masks) in pbar:         \n","        images = images.to(device, dtype=torch.float)\n","        masks  = masks.to(device, dtype=torch.float)\n","        \n","        batch_size = images.size(0)\n","                \n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        with amp.autocast(enabled=True):\n","            y_pred = model(images)\n","            loss   = criterion(y_pred, masks)\n","            loss   = loss / max(1, 32//train_bs)\n","        \n","        scaler.scale(loss).backward()\n","    \n","        if (step+1)%(max(1, 32//train_bs)) == 0:\n","            scaler.step(optimizer)\n","            scaler.update()\n","            \n","            optimizer.zero_grad()\n","            \n","            if scheduler is not None:\n","                scheduler.step()\n","#         optimizer.step()\n","\n","        running_loss += (loss.item() * batch_size)\n","        dataset_size += batch_size\n","        \n","        epoch_loss = running_loss / dataset_size\n","        \n","        mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0\n","        current_lr = optimizer.param_groups[0]['lr']\n","        pbar.set_postfix(train_loss=f'{epoch_loss:0.4f}',\n","                        lr=f'{current_lr:0.5f}',\n","                        gpu_mem=f'{mem:0.2f} GB')\n","    torch.cuda.empty_cache()\n","    gc.collect()\n","    \n","    return epoch_loss"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-02-16T22:20:08.307655Z","iopub.status.busy":"2024-02-16T22:20:08.307313Z","iopub.status.idle":"2024-02-16T22:20:08.318267Z","shell.execute_reply":"2024-02-16T22:20:08.317265Z","shell.execute_reply.started":"2024-02-16T22:20:08.307630Z"},"trusted":true},"outputs":[],"source":["@torch.no_grad()\n","def valid_one_epoch(model, dataloader, device, epoch):\n","    model.eval()\n","    \n","    dataset_size = 0\n","    running_loss = 0.0\n","    \n","    val_scores = []\n","    \n","    pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc='Valid ')\n","    for step, (images, masks) in pbar:        \n","        images  = images.to(device, dtype=torch.float)\n","        masks   = masks.to(device, dtype=torch.float)\n","        \n","        batch_size = images.size(0)\n","        \n","        y_pred  = model(images)\n","        loss    = criterion(y_pred, masks)\n","        \n","        running_loss += (loss.item() * batch_size)\n","        dataset_size += batch_size\n","        \n","        epoch_loss = running_loss / dataset_size\n","        \n","        # Why do we want Sigmoid here?\n","#         y_pred = nn.Sigmoid()(y_pred)  \n","        val_dice = dice_coef(masks, y_pred).cpu().detach().numpy()\n","        val_jaccard = iou_coef(masks, y_pred).cpu().detach().numpy()\n","#         val_hausdorff = hausdorff_distance(masks, y_pred).detach().cpu().numpy()\n","        val_scores.append([val_dice, val_jaccard])\n","            \n","        mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0\n","        current_lr = optimizer.param_groups[0]['lr']\n","        pbar.set_postfix(valid_loss=f'{epoch_loss:0.4f}',\n","                        lr=f'{current_lr:0.5f}',\n","                        gpu_memory=f'{mem:0.2f} GB')\n","    val_scores  = np.mean(val_scores, axis=0)\n","    torch.cuda.empty_cache()\n","    gc.collect()\n","\n","    return epoch_loss, val_scores"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-02-16T22:20:09.944144Z","iopub.status.busy":"2024-02-16T22:20:09.943389Z","iopub.status.idle":"2024-02-16T22:20:09.956981Z","shell.execute_reply":"2024-02-16T22:20:09.955859Z","shell.execute_reply.started":"2024-02-16T22:20:09.944112Z"},"trusted":true},"outputs":[],"source":["def run_training(model, optimizer, scheduler, device, num_epochs):\n","    # To automatically log gradients\n","    cnt = 0\n","    if torch.cuda.is_available():\n","        print(\"cuda: {}\\n\".format(torch.cuda.get_device_name()))\n","    \n","    start = time.time()\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_dice      = -np.inf\n","    best_epoch     = -1\n","    history = defaultdict(list)\n","    \n","    for epoch in range(1, num_epochs + 1):\n","        gc.collect()\n","        print(f'Epoch {epoch}/{num_epochs}', end='')\n","        train_loss = train_one_epoch(model, optimizer, scheduler,\n","                                               dataloader=train_loader, \n","                                               device=device, epoch=epoch)\n","\n","        val_loss, val_scores = valid_one_epoch(model, valid_loader, \n","                                                 device=device, \n","                                                 epoch=epoch)\n","        val_dice, val_jaccard = val_scores\n","    \n","        history['Train Loss'].append(train_loss)\n","        history['Valid Loss'].append(val_loss)\n","        history['Valid Dice'].append(val_dice)\n","        history['Valid Jaccard'].append(val_jaccard)\n","        \n","        # Log the metrics\n","        print(f'Valid Dice: {val_dice:0.4f} | Valid Jaccard: {val_jaccard:0.4f}')\n","        \n","        # deep copy the model\n","        if val_dice > best_dice:\n","            cnt = 0\n","#             print(f\"{c_}Valid Score Improved ({best_dice:0.4f} ---> {val_dice:0.4f})\")\n","            print(f\"Valid Score Improved ({best_dice:0.4f} ---> {val_dice:0.4f})\")\n","            best_dice    = val_dice\n","            best_jaccard = val_jaccard\n","            best_epoch   = epoch\n","            #run.summary[\"Best Dice\"]    = best_dice\n","           # run.summary[\"Best Jaccard\"] = best_jaccard\n","           # run.summary[\"Best Epoch\"]   = best_epoch\n","            best_model_wts = copy.deepcopy(model.state_dict())\n","            PATH = f\"best_epoch-{fold:02d}.bin\"\n","            torch.save(model.state_dict(), PATH)\n","            # Save a model file from the current directory\n","#             print(f\"Model Saved{sr_}\")\n","        else:\n","            cnt += 1\n","        \n","        if cnt>2:\n","            # Early stopping. \n","            # Can also apply callback.\n","            return model, history\n","        \n","        last_model_wts = copy.deepcopy(model.state_dict())\n","        PATH = f\"last_epoch-{fold:02d}.bin\"\n","#         torch.save(model.state_dict(), PATH)\n","            \n","        print(); print()\n","    \n","    end = time.time()\n","    time_elapsed = end - start\n","    print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n","        time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60))\n","    print(\"Best Score: {:.4f}\".format(best_jaccard))\n","    \n","    # load best model weights\n","    model.load_state_dict(best_model_wts)\n","    \n","    return model, history"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2024-02-16T22:20:11.210450Z","iopub.status.busy":"2024-02-16T22:20:11.209804Z","iopub.status.idle":"2024-02-16T22:20:11.218300Z","shell.execute_reply":"2024-02-16T22:20:11.217295Z","shell.execute_reply.started":"2024-02-16T22:20:11.210420Z"},"trusted":true},"outputs":[],"source":["def fetch_scheduler(optimizer, scheduler):\n","    if scheduler == 'CosineAnnealingLR':\n","        scheduler = lr_scheduler.CosineAnnealingLR(optimizer,T_max=T_max, \n","                                                   eta_min=min_lr)\n","    elif scheduler == 'CosineAnnealingWarmRestarts':\n","        scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer,T_0=T_0, \n","                                                             eta_min=min_lr)\n","    elif scheduler == 'ReduceLROnPlateau':\n","        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer,\n","                                                   mode='min',\n","                                                   factor=0.1,\n","                                                   patience=7,\n","                                                   threshold=0.0001,\n","                                                   min_lr=min_lr,)\n","    elif scheduler == 'ExponentialLR':\n","        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.85)\n","    elif scheduler == None:\n","        return None\n","        \n","    return scheduler"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2024-02-16T22:20:23.592657Z","iopub.status.busy":"2024-02-16T22:20:23.591942Z","iopub.status.idle":"2024-02-16T23:56:37.308579Z","shell.execute_reply":"2024-02-16T23:56:37.307565Z","shell.execute_reply.started":"2024-02-16T22:20:23.592625Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["###################################\n","######### Fold: 0\n","###################################\n","Epoch 1/10"]},{"name":"stderr","output_type":"stream","text":["Train :   0%|          | 0/956 [00:06<?, ?it/s]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[32], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2e-6\u001b[39m)\n\u001b[1;32m      6\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m fetch_scheduler(optimizer, scheduler)\n\u001b[0;32m----> 7\u001b[0m model, history \u001b[38;5;241m=\u001b[39m \u001b[43mrun_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[30], line 16\u001b[0m, in \u001b[0;36mrun_training\u001b[0;34m(model, optimizer, scheduler, device, num_epochs)\u001b[0m\n\u001b[1;32m     14\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m val_loss, val_scores \u001b[38;5;241m=\u001b[39m valid_one_epoch(model, valid_loader, \n\u001b[1;32m     21\u001b[0m                                          device\u001b[38;5;241m=\u001b[39mdevice, \n\u001b[1;32m     22\u001b[0m                                          epoch\u001b[38;5;241m=\u001b[39mepoch)\n\u001b[1;32m     23\u001b[0m val_dice, val_jaccard \u001b[38;5;241m=\u001b[39m val_scores\n","Cell \u001b[0;32mIn[28], line 60\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, optimizer, scheduler, dataloader, device, epoch)\u001b[0m\n\u001b[1;32m     57\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m amp\u001b[38;5;241m.\u001b[39mautocast(enabled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m---> 60\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     loss   \u001b[38;5;241m=\u001b[39m criterion(y_pred, masks)\n\u001b[1;32m     62\u001b[0m     loss   \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m32\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mtrain_bs)\n","File \u001b[0;32m~/Desktop/Capstone/Image-Segmentation-for-Gastrointestinal-Tract-Cancer/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Desktop/Capstone/Image-Segmentation-for-Gastrointestinal-Tract-Cancer/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/Desktop/Capstone/Image-Segmentation-for-Gastrointestinal-Tract-Cancer/u_net.py:69\u001b[0m, in \u001b[0;36mUNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     66\u001b[0m x5 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_conv_3(x4)\n\u001b[1;32m     67\u001b[0m x6 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool_3(x5)\n\u001b[0;32m---> 69\u001b[0m x7 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_conv_4(x6)\n\u001b[1;32m     70\u001b[0m x8 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool_4(x7)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Bottleneck\u001b[39;00m\n","File \u001b[0;32m~/Desktop/Capstone/Image-Segmentation-for-Gastrointestinal-Tract-Cancer/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Desktop/Capstone/Image-Segmentation-for-Gastrointestinal-Tract-Cancer/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/Desktop/Capstone/Image-Segmentation-for-Gastrointestinal-Tract-Cancer/u_net.py:19\u001b[0m, in \u001b[0;36mDoubleConv.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdouble_conv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Desktop/Capstone/Image-Segmentation-for-Gastrointestinal-Tract-Cancer/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Desktop/Capstone/Image-Segmentation-for-Gastrointestinal-Tract-Cancer/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/Desktop/Capstone/Image-Segmentation-for-Gastrointestinal-Tract-Cancer/.venv/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n","File \u001b[0;32m~/Desktop/Capstone/Image-Segmentation-for-Gastrointestinal-Tract-Cancer/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Desktop/Capstone/Image-Segmentation-for-Gastrointestinal-Tract-Cancer/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/Desktop/Capstone/Image-Segmentation-for-Gastrointestinal-Tract-Cancer/.venv/lib/python3.9/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Desktop/Capstone/Image-Segmentation-for-Gastrointestinal-Tract-Cancer/.venv/lib/python3.9/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["for fold in range(1):\n","    print(f'#'*35)\n","    print(f'######### Fold: {fold}')\n","    print(f'#'*35)\n","    optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=2e-6)\n","    scheduler = fetch_scheduler(optimizer, scheduler)\n","    model, history = run_training(model, optimizer, scheduler,\n","                                  device=device,\n","                                  num_epochs=10)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-02-16T21:24:29.731314Z","iopub.status.idle":"2024-02-16T21:24:29.731661Z","shell.execute_reply":"2024-02-16T21:24:29.731512Z","shell.execute_reply.started":"2024-02-16T21:24:29.731494Z"},"trusted":true},"outputs":[],"source":["def load_model(path):\n","    model.load_state_dict(torch.load(path))\n","    model.eval()\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-02-16T21:24:29.733688Z","iopub.status.idle":"2024-02-16T21:24:29.734063Z","shell.execute_reply":"2024-02-16T21:24:29.733905Z","shell.execute_reply.started":"2024-02-16T21:24:29.733889Z"},"trusted":true},"outputs":[],"source":["test_dataset = BuildDataset(df_train[df_train.index.isin(valid_ids)], \n","                            transforms=data_transforms['valid'])\n","test_loader  = DataLoader(test_dataset, batch_size=5, \n","                          num_workers=4, shuffle=False, pin_memory=True)\n","\n","imgs, msks =  next(iter(test_loader))\n","\n","imgs = imgs.to(device, dtype=torch.float)\n","\n","preds = []\n","for fold in range(1):\n","    model = load_model(f\"best_epoch-{fold:02d}.bin\")\n","    with torch.no_grad():\n","        pred = model(imgs)\n","        pred = (nn.Sigmoid()(pred)>0.5).double()\n","    preds.append(pred)\n","    \n","imgs  = imgs.cpu().detach()\n","preds = torch.mean(torch.stack(preds, dim=0), dim=0).cpu().detach()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-02-16T21:24:29.735651Z","iopub.status.idle":"2024-02-16T21:24:29.736029Z","shell.execute_reply":"2024-02-16T21:24:29.735871Z","shell.execute_reply.started":"2024-02-16T21:24:29.735836Z"},"trusted":true},"outputs":[],"source":["def plot_batch(imgs, msks, size=3):\n","    plt.figure(figsize=(5*5, 5))\n","    for idx in range(size):\n","        plt.subplot(1, 5, idx+1)\n","        img = imgs[idx,].permute((1, 2, 0)).numpy()*255.0\n","        img = img.astype('uint8')\n","        msk = msks[idx,].permute((1, 2, 0)).numpy()*255.0\n","        show_img_train(img, msk)\n","    plt.tight_layout()\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-02-16T21:24:29.737758Z","iopub.status.idle":"2024-02-16T21:24:29.738157Z","shell.execute_reply":"2024-02-16T21:24:29.738003Z","shell.execute_reply.started":"2024-02-16T21:24:29.737988Z"},"trusted":true},"outputs":[],"source":["def show_img_train(img, mask=None):\n","    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n","#     img = clahe.apply(img)\n","#     plt.figure(figsize=(10,10))\n","    plt.imshow(img, cmap='bone')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-02-16T21:24:29.739153Z","iopub.status.idle":"2024-02-16T21:24:29.739495Z","shell.execute_reply":"2024-02-16T21:24:29.739339Z","shell.execute_reply.started":"2024-02-16T21:24:29.739324Z"},"trusted":true},"outputs":[],"source":["plt.imshow(preds[0].transpose(2,1))\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-02-16T21:24:29.740474Z","iopub.status.idle":"2024-02-16T21:24:29.740800Z","shell.execute_reply":"2024-02-16T21:24:29.740654Z","shell.execute_reply.started":"2024-02-16T21:24:29.740639Z"},"trusted":true},"outputs":[],"source":["plot_batch(imgs, preds, size=5)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-02-16T21:24:29.742201Z","iopub.status.idle":"2024-02-16T21:24:29.742543Z","shell.execute_reply":"2024-02-16T21:24:29.742392Z","shell.execute_reply.started":"2024-02-16T21:24:29.742377Z"},"trusted":true},"outputs":[],"source":["preds"]},{"cell_type":"markdown","metadata":{},"source":["## Train On-Folds"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":18,"metadata":{"trusted":true},"outputs":[],"source":["import plotly.express as px\n","import numpy as np\n","tmp = np.load('pred_arr.txt')\n","# px.imshow(tmp)"]},{"cell_type":"code","execution_count":19,"metadata":{"trusted":true},"outputs":[{"ename":"TypeError","evalue":"ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mpx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtmp\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32ms:\\DS 5500 - Capstone\\capstone\\Lib\\site-packages\\plotly\\express\\_imshow.py:379\u001b[0m, in \u001b[0;36mimshow\u001b[1;34m(img, zmin, zmax, origin, labels, x, y, animation_frame, facet_col, facet_col_wrap, facet_col_spacing, facet_row_spacing, color_continuous_scale, color_continuous_midpoint, range_color, title, template, width, height, aspect, contrast_rescaling, binary_string, binary_backend, binary_compression_level, binary_format, text_auto)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;66;03m# For uint8 data and infer we let zmin and zmax to be None if passed as None\u001b[39;00m\n\u001b[0;32m    378\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m zmax \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m img\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39muint8:\n\u001b[1;32m--> 379\u001b[0m         zmax \u001b[38;5;241m=\u001b[39m \u001b[43m_infer_zmax_from_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m zmin \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m zmax \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    381\u001b[0m         zmin \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n","File \u001b[1;32ms:\\DS 5500 - Capstone\\capstone\\Lib\\site-packages\\plotly\\express\\_imshow.py:45\u001b[0m, in \u001b[0;36m_infer_zmax_from_type\u001b[1;34m(img)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _integer_ranges[dt][\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 45\u001b[0m     im_max \u001b[38;5;241m=\u001b[39m img[\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misfinite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m]\u001b[38;5;241m.\u001b[39mmax()\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m im_max \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m*\u001b[39m rtol:\n\u001b[0;32m     47\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n","\u001b[1;31mTypeError\u001b[0m: ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''"]}],"source":["px.imshow(tmp)"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"data":{"text/plain":["array('[[[[1. 1. 1. ... 1. 1. 1.]\\n   [1. 1. 1. ... 1. 1. 1.]\\n   [1. 1. 1. ... 1. 1. 1.]\\n   ...\\n   [1. 1. 1. ... 1. 1. 1.]\\n   [1. 1. 1. ... 1. 1. 1.]\\n   [1. 1. 1. ... 1. 1. 1.]]\\n\\n  [[1. 1. 1. ... 1. 1. 1.]\\n   [1. 1. 1. ... 1. 1. 1.]\\n   [1. 1. 1. ... 1. 1. 1.]\\n   ...\\n   [1. 1. 1. ... 1. 1. 1.]\\n   [1. 1. 1. ... 1. 1. 1.]\\n   [1. 1. 1. ... 1. 1. 1.]]\\n\\n  [[1. 1. 1. ... 1. 1. 1.]\\n   [1. 1. 1. ... 1. 1. 1.]\\n   [1. 1. 1. ... 1. 1. 1.]\\n   ...\\n   [1. 1. 1. ... 1. 1. 1.]\\n   [1. 1. 1. ... 1. 1. 1.]\\n   [1. 1. 1. ... 1. 1. 1.]]]]',\n","      dtype='<U513')"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["tmp"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":3495119,"sourceId":27923,"sourceType":"competition"}],"dockerImageVersionId":30648,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":4}
