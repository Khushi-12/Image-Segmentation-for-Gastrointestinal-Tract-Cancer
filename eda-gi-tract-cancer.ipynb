{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":27923,"databundleVersionId":3495119,"sourceType":"competition"}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Imports","metadata":{"execution":{"iopub.status.busy":"2024-02-16T21:09:05.333225Z","iopub.execute_input":"2024-02-16T21:09:05.333606Z","iopub.status.idle":"2024-02-16T21:09:05.338442Z","shell.execute_reply.started":"2024-02-16T21:09:05.333567Z","shell.execute_reply":"2024-02-16T21:09:05.337501Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"!pip install segmentation_models_pytorch","metadata":{"execution":{"iopub.status.busy":"2024-02-16T21:49:04.669270Z","iopub.execute_input":"2024-02-16T21:49:04.669665Z","iopub.status.idle":"2024-02-16T21:49:16.863004Z","shell.execute_reply.started":"2024-02-16T21:49:04.669624Z","shell.execute_reply":"2024-02-16T21:49:16.861932Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"Requirement already satisfied: segmentation_models_pytorch in /opt/conda/lib/python3.10/site-packages (0.3.3)\nRequirement already satisfied: torchvision>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from segmentation_models_pytorch) (0.16.2)\nRequirement already satisfied: pretrainedmodels==0.7.4 in /opt/conda/lib/python3.10/site-packages (from segmentation_models_pytorch) (0.7.4)\nRequirement already satisfied: efficientnet-pytorch==0.7.1 in /opt/conda/lib/python3.10/site-packages (from segmentation_models_pytorch) (0.7.1)\nRequirement already satisfied: timm==0.9.2 in /opt/conda/lib/python3.10/site-packages (from segmentation_models_pytorch) (0.9.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from segmentation_models_pytorch) (4.66.1)\nRequirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (from segmentation_models_pytorch) (9.5.0)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2.1.2)\nRequirement already satisfied: munch in /opt/conda/lib/python3.10/site-packages (from pretrainedmodels==0.7.4->segmentation_models_pytorch) (4.0.0)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from timm==0.9.2->segmentation_models_pytorch) (6.0.1)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from timm==0.9.2->segmentation_models_pytorch) (0.20.3)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from timm==0.9.2->segmentation_models_pytorch) (0.4.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (1.24.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (2.31.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2023.12.2)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->timm==0.9.2->segmentation_models_pytorch) (21.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (2023.11.17)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub->timm==0.9.2->segmentation_models_pytorch) (3.1.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Imports\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\ntqdm.pandas()\n\nimport glob\nimport re\nimport plotly.express as pxy\nfrom matplotlib.patches import Rectangle\nfrom matplotlib.colors import LinearSegmentedColormap\nimport torch\n\npd.options.plotting.backend = \"plotly\"\nimport random\nfrom glob import glob\nimport os, shutil\nimport time\nimport copy\nimport joblib\nfrom collections import defaultdict\nimport gc\nfrom IPython import display as ipd\nfrom torch.optim import lr_scheduler\n\n# visualization\nimport cv2\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle\n\n# Sklearn\nfrom sklearn.model_selection import StratifiedKFold, KFold, StratifiedGroupKFold\n\n# PyTorch \nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda import amp\nimport segmentation_models_pytorch as smp\nimport timm\n\n# Albumentations for augmentations\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nimport rasterio\nfrom joblib import Parallel, delayed\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# For descriptive error messages\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n\n\nseed          = 42\nmodel_name    = 'Unet'\ntrain_bs      = 32\nvalid_bs      = train_bs*2\nimg_size      = (224, 224)\nepochs        = 5\nlr            = 2e-3\nscheduler     = 'CosineAnnealingLR'\nmin_lr        = 1e-6\nT_max         = int(30000/train_bs*epochs)+50\nT_0           = 25\nwarmup_epochs = 0\nwd            = 1e-6\nn_accumulate  = max(1, 32//train_bs)\nn_fold        = 5\nfold_selected = 1\nnum_classes   = 3\ndevice        = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2024-02-16T21:49:20.706685Z","iopub.execute_input":"2024-02-16T21:49:20.707634Z","iopub.status.idle":"2024-02-16T21:49:20.722564Z","shell.execute_reply.started":"2024-02-16T21:49:20.707594Z","shell.execute_reply":"2024-02-16T21:49:20.721575Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/uw-madison-gi-tract-image-segmentation/train.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-02-16T21:49:21.213667Z","iopub.execute_input":"2024-02-16T21:49:21.214051Z","iopub.status.idle":"2024-02-16T21:49:21.530473Z","shell.execute_reply.started":"2024-02-16T21:49:21.214019Z","shell.execute_reply":"2024-02-16T21:49:21.529450Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"train_df.info()","metadata":{"execution":{"iopub.status.busy":"2024-02-16T21:49:22.085726Z","iopub.execute_input":"2024-02-16T21:49:22.086127Z","iopub.status.idle":"2024-02-16T21:49:22.126308Z","shell.execute_reply.started":"2024-02-16T21:49:22.086096Z","shell.execute_reply":"2024-02-16T21:49:22.125396Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 115488 entries, 0 to 115487\nData columns (total 3 columns):\n #   Column        Non-Null Count   Dtype \n---  ------        --------------   ----- \n 0   id            115488 non-null  object\n 1   class         115488 non-null  object\n 2   segmentation  33913 non-null   object\ndtypes: object(3)\nmemory usage: 2.6+ MB\n","output_type":"stream"}]},{"cell_type":"code","source":"train_df.describe()","metadata":{"execution":{"iopub.status.busy":"2024-02-16T21:49:22.733661Z","iopub.execute_input":"2024-02-16T21:49:22.734059Z","iopub.status.idle":"2024-02-16T21:49:22.860829Z","shell.execute_reply.started":"2024-02-16T21:49:22.734025Z","shell.execute_reply":"2024-02-16T21:49:22.859923Z"},"trusted":true},"execution_count":48,"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"                            id        class  \\\ncount                   115488       115488   \nunique                   38496            3   \ntop     case30_day0_slice_0105  large_bowel   \nfreq                         3        38496   \n\n                                             segmentation  \ncount                                               33913  \nunique                                              33899  \ntop     34757 14 35114 19 35473 21 35831 23 36189 26 3...  \nfreq                                                    2  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>class</th>\n      <th>segmentation</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>115488</td>\n      <td>115488</td>\n      <td>33913</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>38496</td>\n      <td>3</td>\n      <td>33899</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>case30_day0_slice_0105</td>\n      <td>large_bowel</td>\n      <td>34757 14 35114 19 35473 21 35831 23 36189 26 3...</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>3</td>\n      <td>38496</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Utility Functions\ndef get_image_path(base_path, df):\n    '''Gets the case, day, slice_no and path of the dataset (either train or test).\n    base_path: path to train image folder\n    return :: modified dataframe'''\n    \n    digit_pat = r'[0-9]+'\n    # Create case, day and slice columns\n    df[\"case\"] = df['id'].apply(lambda x: re.findall(digit_pat, x.split('_')[0])[0]) # df[\"id\"].apply(lambda x: x.split(\"_\")[0])\n    df[\"day\"] = df['id'].apply(lambda x: re.findall(digit_pat, x.split('_')[1])[0])  # df[\"id\"].apply(lambda x: x.split(\"_\")[1])\n    df[\"slice_no\"] = df[\"id\"].apply(lambda x: x.split(\"_\")[-1])\n\n    df[\"path\"] = 0\n    \n    n = len(df)\n\n    # Loop through entire dataset\n    for k in tqdm(range(n)):\n        data = df.iloc[k, :]\n        segmentation = data.segmentation\n\n        # In case coordinates for healthy tissue are present\n        case = \"case\"+data.case\n        day = 'day'+data.day\n        slice_no = data.slice_no\n        # Change value to the correct one\n        df.loc[k, \"path\"] = glob(f\"{base_path}/{case}/{case}_{day}/scans/slice_{slice_no}*\")[0]\n    return df\n\n\ndef get_img_size(x, flag):\n    \n    if x != 0:\n        split = x.split(\"_\")\n        width = split[3]\n        height = split[4]\n    \n        if flag == \"width\":\n            return int(width)\n        elif flag == \"height\":\n            return int(height)\n    \n    return 0\n\ndef get_pixel_size(x, flag):\n    \n    if x != 0:\n        split = x.split(\"_\")\n        width = split[-2]\n        height = \".\".join(split[-1].split(\".\")[:-1])\n    \n        if flag == \"width\":\n            return float(width)\n        elif flag == \"height\":\n            return float(height)\n    \n    return 0\n\ndef CustomCmap(rgb_color):\n\n    r1,g1,b1 = rgb_color\n\n    cdict = {'red': ((0, r1, r1),\n                   (1, r1, r1)),\n           'green': ((0, g1, g1),\n                    (1, g1, g1)),\n           'blue': ((0, b1, b1),\n                   (1, b1, b1))}\n\n    cmap = LinearSegmentedColormap('custom_cmap', cdict)\n    return cmap\n\n\ndef show_sample_images(sample_paths):\n    '''Displays simple images (without mask).'''\n\n    # Get additional info from the path\n    case_name = [info.split(\"_\")[0][-7:] for info in sample_paths]\n    day_name = [info.split(\"_\")[1].split(\"/\")[0] for info in sample_paths]\n    slice_name = [info.split(\"_\")[2] for info in sample_paths]\n\n\n    # Plot\n    fig, axs = plt.subplots(2, 5, figsize=(23, 8))\n    axs = axs.flatten()\n\n    for k, path in enumerate(sample_paths):\n        \n        title = f\"{k+1}. {case_name[k]} - {day_name[k]} - {slice_name[k]}\"\n        axs[k].set_title(title, fontsize = 14, \n                         color = my_colors[-1], weight='bold')\n        axs[k].imshow(img)\n        axs[k].axis(\"off\")\n\n    plt.tight_layout()\n    plt.show()\n    \ndef mask_from_segmentation(segment, shape):\n    segm = np.asarray(segment.split(), dtype=int)\n\n   \n    # Get start point and length between points\n    start_point = segm[0::2] - 1\n    length_point = segm[1::2]\n\n    # Compute the location of each endpoint\n    end_point = start_point + length_point\n\n    # Create an empty list mask the size of the original image\n    # take onl\n    case_mask = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n\n    # Change pixels from 0 to 1 that are within the segmentation\n    for start, end in zip(start_point, end_point):\n        case_mask[start:end] = 1\n\n    case_mask = case_mask.reshape((shape[0], shape[1]))    \n    return case_mask\n\ndef plot_original_mask(img, mask, alpha=1):\n\n    # Change pixels - when 1 make True, when 0 make NA\n    mask = np.ma.masked_where(mask == 0, mask)\n\n    # Split the channels\n    mask_largeB = mask[:, :, 0]\n    mask_smallB = mask[:, :, 1]\n    mask_stomach = mask[:, :, 2]\n\n    # Plot the 2 images (Original and with Mask)\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 10))\n\n    # Original\n    ax1.set_title(\"Original Image\")\n    ax1.imshow(img)\n    ax1.axis(\"off\")\n\n    # With Mask\n    ax2.set_title(\"Image with Mask\")\n    ax2.imshow(img)\n    ax2.imshow(mask_largeB, interpolation='none', cmap=CMAP1, alpha=alpha)\n    ax2.imshow(mask_smallB, interpolation='none', cmap=CMAP2, alpha=alpha)\n    ax2.imshow(mask_stomach, interpolation='none',cmap=CMAP3, alpha=alpha)\n    ax2.legend(legend_colors, ['large_bowel', 'small_bowel', 'stomach'])\n    ax2.axis(\"off\")\n    \n#     fig.savefig('foo.png', dpi=500)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-16T21:49:23.546476Z","iopub.execute_input":"2024-02-16T21:49:23.546814Z","iopub.status.idle":"2024-02-16T21:49:23.573505Z","shell.execute_reply.started":"2024-02-16T21:49:23.546786Z","shell.execute_reply":"2024-02-16T21:49:23.572461Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"mask_colors = [(1.0, 0.7, 0.1), (1.0, 0.5, 1.0), (1.0, 0.22, 0.099)]\nlegend_colors = [Rectangle((0,0),1,1, color=color) for color in mask_colors]\nlabels = [\"Large Bowel\", \"Small Bowel\", \"Stomach\"]\n\nCMAP1 = CustomCmap(mask_colors[0])\nCMAP2 = CustomCmap(mask_colors[1])\nCMAP3 = CustomCmap(mask_colors[2])","metadata":{"execution":{"iopub.status.busy":"2024-02-16T21:49:24.421614Z","iopub.execute_input":"2024-02-16T21:49:24.422078Z","iopub.status.idle":"2024-02-16T21:49:24.430341Z","shell.execute_reply.started":"2024-02-16T21:49:24.422040Z","shell.execute_reply":"2024-02-16T21:49:24.429229Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"base_path = \"../input/uw-madison-gi-tract-image-segmentation/train\"\n\n# # Prep and save file\ntrain_data = get_image_path(base_path, df=train_df)\n\ntrain_data[\"image_width\"] = train_data[\"path\"].apply(lambda x: int(x[:-4].rsplit(\"_\",4)[1]))\ntrain_data[\"image_height\"] = train_data[\"path\"].apply(lambda x: int(x[:-4].rsplit(\"_\",4)[2]))\n\ntrain_data[\"pixel_width\"] = train_data[\"path\"].apply(lambda x: int(x[:-4].rsplit(\"_\",4)[1]))\ntrain_data[\"pixel_height\"] = train_data[\"path\"].apply(lambda x: get_pixel_size(x, \"height\"))\n\ntrain_data['slice_no'] = train_data['slice_no'].apply(lambda x: x[0])\n\n\ntrain_data['case'] = train_data['case'].astype(int)\n\ndf_train = pd.DataFrame({'id':train_data['id'][::3]})\n\ndf_train['large_bowel'] = train_data['segmentation'][::3].values\ndf_train['small_bowel'] = train_data['segmentation'][1::3].values\ndf_train['stomach'] = train_data['segmentation'][2::3].values\n\ndf_train['path'] = train_data['path'][::3].values\ndf_train['case'] = train_data['case'][::3].values\ndf_train['day'] = train_data['day'][::3].values\ndf_train['slice'] = train_data['slice_no'][::3].values\ndf_train['width'] = train_data['image_width'][::3].values\ndf_train['height'] = train_data['image_height'][::3].values\n\n\ndf_train.reset_index(inplace=True,drop=True)\ndf_train.fillna('',inplace=True); \ndf_train['count'] = np.sum(df_train.iloc[:,1:4]!='',axis=1).values\ndf_train.sample(5)","metadata":{"execution":{"iopub.status.busy":"2024-02-16T21:49:25.197506Z","iopub.execute_input":"2024-02-16T21:49:25.197881Z","iopub.status.idle":"2024-02-16T21:51:09.196228Z","shell.execute_reply.started":"2024-02-16T21:49:25.197829Z","shell.execute_reply":"2024-02-16T21:51:09.194797Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stderr","text":"100%|██████████| 115488/115488 [01:42<00:00, 1126.17it/s]\n","output_type":"stream"},{"execution_count":51,"output_type":"execute_result","data":{"text/plain":"                             id  \\\n30417   case18_day19_slice_0066   \n2315   case139_day16_slice_0012   \n31908   case144_day0_slice_0037   \n24834   case43_day20_slice_0131   \n14138     case6_day0_slice_0059   \n\n                                             large_bowel small_bowel  \\\n30417  46685 7 47043 10 47402 12 47761 14 48120 15 48...               \n2315                                                                   \n31908                                                                  \n24834                                                                  \n14138                                                                  \n\n                                                 stomach  \\\n30417  36558 11 36915 19 37273 24 37632 27 37990 30 3...   \n2315                                                       \n31908                                                      \n24834                                                      \n14138  42721 8 43079 12 43438 14 43797 16 44156 17 44...   \n\n                                                    path  case day slice  \\\n30417  ../input/uw-madison-gi-tract-image-segmentatio...    18  19     0   \n2315   ../input/uw-madison-gi-tract-image-segmentatio...   139  16     0   \n31908  ../input/uw-madison-gi-tract-image-segmentatio...   144   0     0   \n24834  ../input/uw-madison-gi-tract-image-segmentatio...    43  20     0   \n14138  ../input/uw-madison-gi-tract-image-segmentatio...     6   0     0   \n\n       width  height  count  \n30417    360     310      2  \n2315     266     266      0  \n31908    266     266      0  \n24834    266     266      0  \n14138    360     310      1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>large_bowel</th>\n      <th>small_bowel</th>\n      <th>stomach</th>\n      <th>path</th>\n      <th>case</th>\n      <th>day</th>\n      <th>slice</th>\n      <th>width</th>\n      <th>height</th>\n      <th>count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>30417</th>\n      <td>case18_day19_slice_0066</td>\n      <td>46685 7 47043 10 47402 12 47761 14 48120 15 48...</td>\n      <td></td>\n      <td>36558 11 36915 19 37273 24 37632 27 37990 30 3...</td>\n      <td>../input/uw-madison-gi-tract-image-segmentatio...</td>\n      <td>18</td>\n      <td>19</td>\n      <td>0</td>\n      <td>360</td>\n      <td>310</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2315</th>\n      <td>case139_day16_slice_0012</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>../input/uw-madison-gi-tract-image-segmentatio...</td>\n      <td>139</td>\n      <td>16</td>\n      <td>0</td>\n      <td>266</td>\n      <td>266</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>31908</th>\n      <td>case144_day0_slice_0037</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>../input/uw-madison-gi-tract-image-segmentatio...</td>\n      <td>144</td>\n      <td>0</td>\n      <td>0</td>\n      <td>266</td>\n      <td>266</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>24834</th>\n      <td>case43_day20_slice_0131</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>../input/uw-madison-gi-tract-image-segmentatio...</td>\n      <td>43</td>\n      <td>20</td>\n      <td>0</td>\n      <td>266</td>\n      <td>266</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>14138</th>\n      <td>case6_day0_slice_0059</td>\n      <td></td>\n      <td></td>\n      <td>42721 8 43079 12 43438 14 43797 16 44156 17 44...</td>\n      <td>../input/uw-madison-gi-tract-image-segmentatio...</td>\n      <td>6</td>\n      <td>0</td>\n      <td>0</td>\n      <td>360</td>\n      <td>310</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def rle_decode(mask_rle, shape):\n    '''\n    mask_rle: run-length as string formated (start length)\n    shape: (height,width) of array to return\n    Returns numpy array, 1 - mask, 0 - background\n    '''\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n    return img.reshape(shape)  # Needed to align to RLE direction\n\n\n# ref.: https://www.kaggle.com/stainsby/fast-tested-rle\ndef rle_encode(img):\n    '''\n    img: numpy array, 1 - mask, 0 - background\n    Returns run length as string formated\n    '''\n    pixels = img.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)","metadata":{"execution":{"iopub.status.busy":"2024-02-16T21:51:09.197923Z","iopub.execute_input":"2024-02-16T21:51:09.198215Z","iopub.status.idle":"2024-02-16T21:51:09.207131Z","shell.execute_reply.started":"2024-02-16T21:51:09.198190Z","shell.execute_reply":"2024-02-16T21:51:09.206081Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"class BuildDataset(torch.utils.data.Dataset):\n    def __init__(self, df, subset='train', transforms=None):\n        self.df = df\n        self.subset = subset\n        self.transforms = transforms\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        masks = np.zeros((224,224,3), dtype=np.float32)\n        img_path = self.df['path'].iloc[index]\n        w = self.df['width'].iloc[index]\n        h = self.df['height'].iloc[index]\n        img = self.__load_img(img_path)\n        if self.subset=='train':\n            for k,j in zip([0,1,2],[\"large_bowel\",\"small_bowel\",\"stomach\"]):\n                rles=self.df[j].iloc[index]\n                mask = rle_decode(rles, shape=(h, w, 1))\n                mask = cv2.resize(mask, (224,224))\n                masks[:,:,k] = mask\n        \n        masks = masks.transpose(2, 0, 1)\n        img = img.transpose(2, 0, 1)\n        \n        if self.subset=='train': return torch.tensor(img), torch.tensor(masks)\n        else: return torch.tensor(img)\n\n    def __load_img(self, img_path):\n        img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n        img = (img - img.min())/(img.max() - img.min())*255.0 \n        img = cv2.resize(img, (224,224))\n        img = np.tile(img[...,None], [1, 1, 3]) # gray to rgb\n        img = img.astype(np.float32) /255.\n        return img\n    \n    def get_id_mask(self, idx, verbose=False):\n        '''Returns a mask for each case ID. If no segmentation was found, the mask will be empty\n        - meaning formed by only 0\n        ID: the case ID from the train.csv file\n        verbose: True if we want any prints\n        return: segmentation mask'''\n\n        train = self.df\n        \n        # ~~~ Get the data ~~~\n        # Get the portion of dataframe where we have ONLY the speciffied ID\n        #index_data = train[train['id']==id].reset_index(drop=True)\n        for row in train[train.index==idx].iterrows():\n            row = row[1]\n\n            # Split the dataframe into 3 series of observations\n            # each for one speciffic class - \"large_bowel\", \"small_bowel\", \"stomach\"\n            observations = [index_data.loc[k, :] for k in range(3)]\n        \n        # ~~~ Create the mask ~~~\n        # Get the maximum height out of all observations\n        # if max == 0 then no class has a segmentation\n        # otherwise we keep the length of the mask\n        max_height = np.max([obs.image_height for obs in observations])\n        max_width = np.max([obs.image_width for obs in observations])\n\n        # Get shape of the image\n        # 3 channels of color/classes\n        shape = (max_height, max_width, 3)\n\n        # Create an empty mask with the shape of the image\n        mask = np.zeros(shape, dtype=np.uint8)\n\n        # If there is at least 1 segmentation found in the group of 3 classes\n        if max_height != 0:\n            for k, location in enumerate([\"large_bowel\", \"small_bowel\", \"stomach\"]):\n                observation = observations[k]\n                segmentation = observation.segmentation\n\n                # If a segmentation is found\n                # Append a new channel to the mask\n                if pd.isnull(segmentation) == False:\n                    mask[..., k] = mask_from_segmentation(segmentation, shape)\n\n        return mask","metadata":{"execution":{"iopub.status.busy":"2024-02-16T21:51:09.208559Z","iopub.execute_input":"2024-02-16T21:51:09.208875Z","iopub.status.idle":"2024-02-16T21:51:09.226362Z","shell.execute_reply.started":"2024-02-16T21:51:09.208833Z","shell.execute_reply":"2024-02-16T21:51:09.225358Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"data_transforms = {\n    \"train\": A.Compose([\n        A.Resize(224,224, interpolation=cv2.INTER_NEAREST),\n        A.Blur(blur_limit=(5, 5), p=1.0),\n        A.Normalize(mean=0.5, std=2, max_pixel_value=255.0)], p=1.0),\n    \"valid\": A.Compose([\n        A.Resize(224,224, interpolation=cv2.INTER_NEAREST),\n        ], p=1.0)\n}","metadata":{"execution":{"iopub.status.busy":"2024-02-16T21:53:54.032007Z","iopub.execute_input":"2024-02-16T21:53:54.032927Z","iopub.status.idle":"2024-02-16T21:53:54.038804Z","shell.execute_reply.started":"2024-02-16T21:53:54.032891Z","shell.execute_reply":"2024-02-16T21:53:54.037717Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"skf = StratifiedGroupKFold(n_splits=n_fold, shuffle=True, random_state=42)\nfor fold, (_, val_idx) in enumerate(skf.split(X=df_train, y=df_train['count'],groups =df_train['case']), 1):\n    df_train.loc[val_idx, 'fold'] = fold\n\ndf_train['fold'] = df_train['fold'].astype(np.uint8)\n\ntrain_ids = df_train[df_train[\"fold\"]!=fold_selected].index\nvalid_ids = df_train[df_train[\"fold\"]==fold_selected].index\n\n# train_ids = train_data.index[:30000]\n# valid_ids = train_data.index[30000:]","metadata":{"execution":{"iopub.status.busy":"2024-02-16T21:53:55.324865Z","iopub.execute_input":"2024-02-16T21:53:55.325505Z","iopub.status.idle":"2024-02-16T21:53:55.494186Z","shell.execute_reply.started":"2024-02-16T21:53:55.325473Z","shell.execute_reply":"2024-02-16T21:53:55.493287Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"train_dataset = BuildDataset(df_train[df_train.index.isin(train_ids)], transforms=data_transforms['train'])\nvalid_dataset = BuildDataset(df_train[df_train.index.isin(valid_ids)], transforms=data_transforms['valid'])\n\ntrain_loader = DataLoader(train_dataset,batch_size=32, num_workers=4, shuffle=True, pin_memory=True, drop_last=False)\n\nvalid_loader = DataLoader(valid_dataset, batch_size=64,num_workers=4, shuffle=False, pin_memory=True)\n\nimgs, msks = next(iter(train_loader))\nimgs.size(), msks.size()","metadata":{"execution":{"iopub.status.busy":"2024-02-16T21:59:46.205544Z","iopub.execute_input":"2024-02-16T21:59:46.206027Z","iopub.status.idle":"2024-02-16T21:59:47.120525Z","shell.execute_reply.started":"2024-02-16T21:59:46.205983Z","shell.execute_reply":"2024-02-16T21:59:47.119420Z"},"trusted":true},"execution_count":58,"outputs":[{"execution_count":58,"output_type":"execute_result","data":{"text/plain":"(torch.Size([32, 3, 224, 224]), torch.Size([32, 3, 224, 224]))"},"metadata":{}}]},{"cell_type":"code","source":"ENCODER = 'efficientnet-b4'\nENCODER_WEIGHTS = 'imagenet'\nCLASSES = ['large_bowel', 'small_bowel', 'stomach']\nACTIVATION = 'softmax' # could be None for logits or 'softmax2d' for multiclass segmentation\n\n# create segmentation model with pretrained encoder\nmodel = smp.Unet(\n    encoder_name=ENCODER,\n    encoder_weights=ENCODER_WEIGHTS,\n    classes=len(CLASSES),\n    activation=ACTIVATION,\n)\n\npreprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)","metadata":{"execution":{"iopub.status.busy":"2024-02-16T21:59:52.232058Z","iopub.execute_input":"2024-02-16T21:59:52.232447Z","iopub.status.idle":"2024-02-16T21:59:52.624245Z","shell.execute_reply.started":"2024-02-16T21:59:52.232414Z","shell.execute_reply":"2024-02-16T21:59:52.623395Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"device = device\nDiceLoss    = smp.losses.DiceLoss(mode='multilabel').to(device)\nBCELoss     = smp.losses.SoftBCEWithLogitsLoss()\nJaccardLoss = smp.losses.JaccardLoss(mode='multilabel')\n\nfrom scipy.spatial.distance import directed_hausdorff\n\ndef dice_coef(y_true, y_pred, thr=0.5, dim=(2,3), epsilon=0.001):\n    y_true = y_true.to(torch.float32)\n    y_pred = (y_pred>thr).to(torch.float32)\n    inter = (y_true*y_pred).sum(dim=dim)\n    den = y_true.sum(dim=dim) + y_pred.sum(dim=dim)\n    dice = ((2*inter)/(den+epsilon)).mean(dim=(1,0))\n    return dice\n\n\n# The training loss goes to nan while evaluating the model through this hausdorff distance.\ndef hausdorff_distance(y_true, y_pred):\n    difference = y_true - y_pred\n    \n    # Square distances using PyTorch einsum\n    square_distances = torch.einsum(\"...i,...i->...\", difference, difference)\n    \n    minimum_square_distance_a_to_b = torch.min(square_distances, dim=-1)[0]\n    \n    # Here we are outputting the mean hausdorff distance.\n    return torch.mean(torch.sqrt(torch.max(minimum_square_distance_a_to_b, dim=-1).values))\n\ndef iou_coef(y_true, y_pred, thr=0.5, dim=(2,3), epsilon=0.001):\n    y_true = y_true.to(torch.float32)\n    y_pred = (y_pred>thr).to(torch.float32)\n    inter = (y_true*y_pred).sum(dim=dim)\n    union = (y_true + y_pred - y_true*y_pred).sum(dim=dim)\n    iou = ((inter+epsilon)/(union+epsilon)).mean(dim=(1,0))\n    return iou\n\ndef criterion(y_pred, y_true):\n    return 0.6*BCELoss(y_true, y_pred) + 0.4*DiceLoss(y_pred, y_true)\n\ndef train_one_epoch(model, optimizer, scheduler, dataloader, device, epoch):\n    model.to(device)\n    model.train()\n    scaler = amp.GradScaler()\n    \n    dataset_size = 0\n    running_loss = 0.0\n    \n    pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc='Train ')\n    for step, (images, masks) in pbar:         \n        images = images.to(device, dtype=torch.float)\n        masks  = masks.to(device, dtype=torch.float)\n        \n        batch_size = images.size(0)\n                \n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        with amp.autocast(enabled=True):\n            y_pred = model(images)\n            loss   = criterion(y_pred, masks)\n            loss   = loss / max(1, 32//train_bs)\n        \n        scaler.scale(loss).backward()\n    \n        if (step+1)%(max(1, 32//train_bs)) == 0:\n            scaler.step(optimizer)\n            scaler.update()\n            \n            optimizer.zero_grad()\n            \n            if scheduler is not None:\n                scheduler.step()\n#         optimizer.step()\n\n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n        \n        epoch_loss = running_loss / dataset_size\n        \n        mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0\n        current_lr = optimizer.param_groups[0]['lr']\n        pbar.set_postfix(train_loss=f'{epoch_loss:0.4f}',\n                        lr=f'{current_lr:0.5f}',\n                        gpu_mem=f'{mem:0.2f} GB')\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    return epoch_loss","metadata":{"execution":{"iopub.status.busy":"2024-02-16T22:19:54.877587Z","iopub.execute_input":"2024-02-16T22:19:54.878088Z","iopub.status.idle":"2024-02-16T22:19:54.898058Z","shell.execute_reply.started":"2024-02-16T22:19:54.878054Z","shell.execute_reply":"2024-02-16T22:19:54.897072Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"@torch.no_grad()\ndef valid_one_epoch(model, dataloader, device, epoch):\n    model.eval()\n    \n    dataset_size = 0\n    running_loss = 0.0\n    \n    val_scores = []\n    \n    pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc='Valid ')\n    for step, (images, masks) in pbar:        \n        images  = images.to(device, dtype=torch.float)\n        masks   = masks.to(device, dtype=torch.float)\n        \n        batch_size = images.size(0)\n        \n        y_pred  = model(images)\n        loss    = criterion(y_pred, masks)\n        \n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n        \n        epoch_loss = running_loss / dataset_size\n        \n        # Why do we want Sigmoid here?\n#         y_pred = nn.Sigmoid()(y_pred)  \n        val_dice = dice_coef(masks, y_pred).cpu().detach().numpy()\n        val_jaccard = iou_coef(masks, y_pred).cpu().detach().numpy()\n#         val_hausdorff = hausdorff_distance(masks, y_pred).detach().cpu().numpy()\n        val_scores.append([val_dice, val_jaccard])\n            \n        mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0\n        current_lr = optimizer.param_groups[0]['lr']\n        pbar.set_postfix(valid_loss=f'{epoch_loss:0.4f}',\n                        lr=f'{current_lr:0.5f}',\n                        gpu_memory=f'{mem:0.2f} GB')\n    val_scores  = np.mean(val_scores, axis=0)\n    torch.cuda.empty_cache()\n    gc.collect()\n\n    return epoch_loss, val_scores","metadata":{"execution":{"iopub.status.busy":"2024-02-16T22:20:08.307313Z","iopub.execute_input":"2024-02-16T22:20:08.307655Z","iopub.status.idle":"2024-02-16T22:20:08.318267Z","shell.execute_reply.started":"2024-02-16T22:20:08.307630Z","shell.execute_reply":"2024-02-16T22:20:08.317265Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"def run_training(model, optimizer, scheduler, device, num_epochs):\n    # To automatically log gradients\n    cnt = 0\n    if torch.cuda.is_available():\n        print(\"cuda: {}\\n\".format(torch.cuda.get_device_name()))\n    \n    start = time.time()\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_dice      = -np.inf\n    best_epoch     = -1\n    history = defaultdict(list)\n    \n    for epoch in range(1, num_epochs + 1):\n        gc.collect()\n        print(f'Epoch {epoch}/{num_epochs}', end='')\n        train_loss = train_one_epoch(model, optimizer, scheduler,\n                                               dataloader=train_loader, \n                                               device=device, epoch=epoch)\n\n        val_loss, val_scores = valid_one_epoch(model, valid_loader, \n                                                 device=device, \n                                                 epoch=epoch)\n        val_dice, val_jaccard = val_scores\n    \n        history['Train Loss'].append(train_loss)\n        history['Valid Loss'].append(val_loss)\n        history['Valid Dice'].append(val_dice)\n        history['Valid Jaccard'].append(val_jaccard)\n        \n        # Log the metrics\n        print(f'Valid Dice: {val_dice:0.4f} | Valid Jaccard: {val_jaccard:0.4f}')\n        \n        # deep copy the model\n        if val_dice > best_dice:\n            cnt = 0\n#             print(f\"{c_}Valid Score Improved ({best_dice:0.4f} ---> {val_dice:0.4f})\")\n            print(f\"Valid Score Improved ({best_dice:0.4f} ---> {val_dice:0.4f})\")\n            best_dice    = val_dice\n            best_jaccard = val_jaccard\n            best_epoch   = epoch\n            #run.summary[\"Best Dice\"]    = best_dice\n           # run.summary[\"Best Jaccard\"] = best_jaccard\n           # run.summary[\"Best Epoch\"]   = best_epoch\n            best_model_wts = copy.deepcopy(model.state_dict())\n            PATH = f\"best_epoch-{fold:02d}.bin\"\n            torch.save(model.state_dict(), PATH)\n            # Save a model file from the current directory\n#             print(f\"Model Saved{sr_}\")\n        else:\n            cnt += 1\n        \n        if cnt>2:\n            # Early stopping. \n            # Can also apply callback.\n            return model, history\n        \n        last_model_wts = copy.deepcopy(model.state_dict())\n        PATH = f\"last_epoch-{fold:02d}.bin\"\n#         torch.save(model.state_dict(), PATH)\n            \n        print(); print()\n    \n    end = time.time()\n    time_elapsed = end - start\n    print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n        time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60))\n    print(\"Best Score: {:.4f}\".format(best_jaccard))\n    \n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    \n    return model, history","metadata":{"execution":{"iopub.status.busy":"2024-02-16T22:20:09.943389Z","iopub.execute_input":"2024-02-16T22:20:09.944144Z","iopub.status.idle":"2024-02-16T22:20:09.956981Z","shell.execute_reply.started":"2024-02-16T22:20:09.944112Z","shell.execute_reply":"2024-02-16T22:20:09.955859Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"def fetch_scheduler(optimizer, scheduler):\n    if scheduler == 'CosineAnnealingLR':\n        scheduler = lr_scheduler.CosineAnnealingLR(optimizer,T_max=T_max, \n                                                   eta_min=min_lr)\n    elif scheduler == 'CosineAnnealingWarmRestarts':\n        scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer,T_0=T_0, \n                                                             eta_min=min_lr)\n    elif scheduler == 'ReduceLROnPlateau':\n        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer,\n                                                   mode='min',\n                                                   factor=0.1,\n                                                   patience=7,\n                                                   threshold=0.0001,\n                                                   min_lr=min_lr,)\n    elif scheduler == 'ExponentialLR':\n        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.85)\n    elif scheduler == None:\n        return None\n        \n    return scheduler","metadata":{"execution":{"iopub.status.busy":"2024-02-16T22:20:11.209804Z","iopub.execute_input":"2024-02-16T22:20:11.210450Z","iopub.status.idle":"2024-02-16T22:20:11.218300Z","shell.execute_reply.started":"2024-02-16T22:20:11.210420Z","shell.execute_reply":"2024-02-16T22:20:11.217295Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"for fold in range(1):\n    print(f'#'*35)\n    print(f'######### Fold: {fold}')\n    print(f'#'*35)\n    optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=2e-6)\n    scheduler = fetch_scheduler(optimizer, scheduler)\n    model, history = run_training(model, optimizer, scheduler,\n                                  device=device,\n                                  num_epochs=10)","metadata":{"execution":{"iopub.status.busy":"2024-02-16T22:20:23.591942Z","iopub.execute_input":"2024-02-16T22:20:23.592657Z","iopub.status.idle":"2024-02-16T23:56:37.308579Z","shell.execute_reply.started":"2024-02-16T22:20:23.592625Z","shell.execute_reply":"2024-02-16T23:56:37.307565Z"},"trusted":true},"execution_count":64,"outputs":[{"name":"stdout","text":"###################################\n######### Fold: 0\n###################################\ncuda: Tesla P100-PCIE-16GB\n\nEpoch 1/10","output_type":"stream"},{"name":"stderr","text":"Train : 100%|██████████| 956/956 [09:01<00:00,  1.77it/s, gpu_mem=7.77 GB, lr=0.00009, train_loss=0.8092]\nValid : 100%|██████████| 124/124 [00:36<00:00,  3.41it/s, gpu_memory=7.23 GB, lr=0.00009, valid_loss=0.7027]\n","output_type":"stream"},{"name":"stdout","text":"Valid Dice: 0.0848 | Valid Jaccard: 0.1969\nValid Score Improved (-inf ---> 0.0848)\n\n\nEpoch 2/10","output_type":"stream"},{"name":"stderr","text":"Train : 100%|██████████| 956/956 [08:59<00:00,  1.77it/s, gpu_mem=8.02 GB, lr=0.00007, train_loss=0.8087]\nValid : 100%|██████████| 124/124 [00:36<00:00,  3.37it/s, gpu_memory=7.23 GB, lr=0.00007, valid_loss=0.7026]\n","output_type":"stream"},{"name":"stdout","text":"Valid Dice: 0.1017 | Valid Jaccard: 0.3039\nValid Score Improved (0.0848 ---> 0.1017)\n\n\nEpoch 3/10","output_type":"stream"},{"name":"stderr","text":"Train : 100%|██████████| 956/956 [08:58<00:00,  1.77it/s, gpu_mem=8.02 GB, lr=0.00003, train_loss=0.8084]\nValid : 100%|██████████| 124/124 [00:36<00:00,  3.44it/s, gpu_memory=7.23 GB, lr=0.00003, valid_loss=0.7026]\n","output_type":"stream"},{"name":"stdout","text":"Valid Dice: 0.1046 | Valid Jaccard: 0.2636\nValid Score Improved (0.1017 ---> 0.1046)\n\n\nEpoch 4/10","output_type":"stream"},{"name":"stderr","text":"Train : 100%|██████████| 956/956 [08:59<00:00,  1.77it/s, gpu_mem=8.02 GB, lr=0.00001, train_loss=0.8085]\nValid : 100%|██████████| 124/124 [00:36<00:00,  3.43it/s, gpu_memory=7.24 GB, lr=0.00001, valid_loss=0.7026]\n","output_type":"stream"},{"name":"stdout","text":"Valid Dice: 0.1098 | Valid Jaccard: 0.3424\nValid Score Improved (0.1046 ---> 0.1098)\n\n\nEpoch 5/10","output_type":"stream"},{"name":"stderr","text":"Train : 100%|██████████| 956/956 [08:58<00:00,  1.77it/s, gpu_mem=8.02 GB, lr=0.00000, train_loss=0.8086]\nValid : 100%|██████████| 124/124 [00:36<00:00,  3.44it/s, gpu_memory=7.22 GB, lr=0.00000, valid_loss=0.7026]\n","output_type":"stream"},{"name":"stdout","text":"Valid Dice: 0.1086 | Valid Jaccard: 0.3410\n\n\nEpoch 6/10","output_type":"stream"},{"name":"stderr","text":"Train : 100%|██████████| 956/956 [08:59<00:00,  1.77it/s, gpu_mem=8.02 GB, lr=0.00001, train_loss=0.8086]\nValid : 100%|██████████| 124/124 [00:36<00:00,  3.44it/s, gpu_memory=7.33 GB, lr=0.00001, valid_loss=0.7026]\n","output_type":"stream"},{"name":"stdout","text":"Valid Dice: 0.1101 | Valid Jaccard: 0.3401\nValid Score Improved (0.1098 ---> 0.1101)\n\n\nEpoch 7/10","output_type":"stream"},{"name":"stderr","text":"Train : 100%|██████████| 956/956 [08:58<00:00,  1.77it/s, gpu_mem=8.02 GB, lr=0.00004, train_loss=0.8087]\nValid : 100%|██████████| 124/124 [00:36<00:00,  3.44it/s, gpu_memory=7.21 GB, lr=0.00004, valid_loss=0.7027]\n","output_type":"stream"},{"name":"stdout","text":"Valid Dice: 0.1116 | Valid Jaccard: 0.3280\nValid Score Improved (0.1101 ---> 0.1116)\n\n\nEpoch 8/10","output_type":"stream"},{"name":"stderr","text":"Train : 100%|██████████| 956/956 [08:59<00:00,  1.77it/s, gpu_mem=8.03 GB, lr=0.00007, train_loss=0.8085]\nValid : 100%|██████████| 124/124 [00:36<00:00,  3.41it/s, gpu_memory=7.35 GB, lr=0.00007, valid_loss=0.7026]\n","output_type":"stream"},{"name":"stdout","text":"Valid Dice: 0.1093 | Valid Jaccard: 0.3019\n\n\nEpoch 9/10","output_type":"stream"},{"name":"stderr","text":"Train : 100%|██████████| 956/956 [09:02<00:00,  1.76it/s, gpu_mem=8.01 GB, lr=0.00009, train_loss=0.8087]\nValid : 100%|██████████| 124/124 [00:36<00:00,  3.43it/s, gpu_memory=7.22 GB, lr=0.00009, valid_loss=0.7026]\n","output_type":"stream"},{"name":"stdout","text":"Valid Dice: 0.1102 | Valid Jaccard: 0.2332\n\n\nEpoch 10/10","output_type":"stream"},{"name":"stderr","text":"Train : 100%|██████████| 956/956 [08:57<00:00,  1.78it/s, gpu_mem=8.04 GB, lr=0.00010, train_loss=0.8087]\nValid : 100%|██████████| 124/124 [00:35<00:00,  3.46it/s, gpu_memory=7.24 GB, lr=0.00010, valid_loss=0.7026]\n","output_type":"stream"},{"name":"stdout","text":"Valid Dice: 0.1082 | Valid Jaccard: 0.2308\n","output_type":"stream"}]},{"cell_type":"code","source":"def load_model(path):\n    model.load_state_dict(torch.load(path))\n    model.eval()\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-02-16T21:24:29.731314Z","iopub.status.idle":"2024-02-16T21:24:29.731661Z","shell.execute_reply.started":"2024-02-16T21:24:29.731494Z","shell.execute_reply":"2024-02-16T21:24:29.731512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = BuildDataset(df_train[df_train.index.isin(valid_ids)], \n                            transforms=data_transforms['valid'])\ntest_loader  = DataLoader(test_dataset, batch_size=5, \n                          num_workers=4, shuffle=False, pin_memory=True)\n\nimgs, msks =  next(iter(test_loader))\n\nimgs = imgs.to(device, dtype=torch.float)\n\npreds = []\nfor fold in range(1):\n    model = load_model(f\"best_epoch-{fold:02d}.bin\")\n    with torch.no_grad():\n        pred = model(imgs)\n        pred = (nn.Sigmoid()(pred)>0.5).double()\n    preds.append(pred)\n    \nimgs  = imgs.cpu().detach()\npreds = torch.mean(torch.stack(preds, dim=0), dim=0).cpu().detach()","metadata":{"execution":{"iopub.status.busy":"2024-02-16T21:24:29.733688Z","iopub.status.idle":"2024-02-16T21:24:29.734063Z","shell.execute_reply.started":"2024-02-16T21:24:29.733889Z","shell.execute_reply":"2024-02-16T21:24:29.733905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_batch(imgs, msks, size=3):\n    plt.figure(figsize=(5*5, 5))\n    for idx in range(size):\n        plt.subplot(1, 5, idx+1)\n        img = imgs[idx,].permute((1, 2, 0)).numpy()*255.0\n        img = img.astype('uint8')\n        msk = msks[idx,].permute((1, 2, 0)).numpy()*255.0\n        show_img_train(img, msk)\n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-16T21:24:29.735651Z","iopub.status.idle":"2024-02-16T21:24:29.736029Z","shell.execute_reply.started":"2024-02-16T21:24:29.735836Z","shell.execute_reply":"2024-02-16T21:24:29.735871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_img_train(img, mask=None):\n    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n#     img = clahe.apply(img)\n#     plt.figure(figsize=(10,10))\n    plt.imshow(img, cmap='bone')","metadata":{"execution":{"iopub.status.busy":"2024-02-16T21:24:29.737758Z","iopub.status.idle":"2024-02-16T21:24:29.738157Z","shell.execute_reply.started":"2024-02-16T21:24:29.737988Z","shell.execute_reply":"2024-02-16T21:24:29.738003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(preds[0].transpose(2,1))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-16T21:24:29.739153Z","iopub.status.idle":"2024-02-16T21:24:29.739495Z","shell.execute_reply.started":"2024-02-16T21:24:29.739324Z","shell.execute_reply":"2024-02-16T21:24:29.739339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_batch(imgs, preds, size=5)","metadata":{"execution":{"iopub.status.busy":"2024-02-16T21:24:29.740474Z","iopub.status.idle":"2024-02-16T21:24:29.740800Z","shell.execute_reply.started":"2024-02-16T21:24:29.740639Z","shell.execute_reply":"2024-02-16T21:24:29.740654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds","metadata":{"execution":{"iopub.status.busy":"2024-02-16T21:24:29.742201Z","iopub.status.idle":"2024-02-16T21:24:29.742543Z","shell.execute_reply.started":"2024-02-16T21:24:29.742377Z","shell.execute_reply":"2024-02-16T21:24:29.742392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train On-Folds","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}